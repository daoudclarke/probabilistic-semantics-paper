%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{eacl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{color}
\usepackage{listings}
\usepackage{csvsimple}
\usepackage{titleref}
%\usepackage{hyperref}
%\usepackage{natbib}

\renewcommand{\theTitleReference}[2]{\emph{#2}}

%\renewcommand{\cite}{\citep}
%\newcommand{\citet}[1]{\citeauthor{#1} \shortcite{#1}}
\newcommand{\citet}[1]{\newcite{#1}}

\lstset{
         basicstyle=\footnotesize\ttfamily, % Standardschrift
         %numbers=left,               % Ort der Zeilennummern
         numberstyle=\tiny,          % Stil der Zeilennummern
         %stepnumber=2,               % Abstand zwischen den
         %Zeilennummern
         numbersep=5pt,              % Abstand der Nummern zum Text
         tabsize=2,                  % Groesse von Tabs
         extendedchars=true,         %
         breaklines=true,            % Zeilen werden Umgebrochen
         keywordstyle=\color{black},
         frame=b,         
 %        keywordstyle=[1]\textbf,    % Stil der Keywords
 %        keywordstyle=[2]\textbf,    %
 %        keywordstyle=[3]\textbf,    %
 %        keywordstyle=[4]\textbf,   \sqrt{\sqrt{}} %
         stringstyle=\color{white}\ttfamily, % Farbe der String
         showspaces=false,           % Leerzeichen anzeigen ?
         showtabs=false,             % Tabs anzeigen ?
         xleftmargin=17pt,
         framexleftmargin=17pt,
         framexrightmargin=5pt,
         framexbottommargin=4pt,
         %backgroundcolor=\color{lightgray},
         showstringspaces=false,      % Leerzeichen in Strings anzeigen
         language=prolog
 }


 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Probabilistic Semantics for Natural Language}
\author{Daoud Clarke and Bill Keller\\
Department of Informatics\\
University of Sussex\\
Falmer, Brighton, UK\\
}
\maketitle
\begin{abstract}
% \begin{quote}
Distributional accounts of meaning
represent words as vectors or probability distributions over
contexts. Such approaches differ from compositional, model-theoretic treatments of meaning
such as Montague semantics. Recently, researchers have begun to address the problem of  developing a unified account of natural language semantics that combines the strengths of both the distributional and compositional approaches. This paper presents a new formalism, {\em probabilistic semantics\/}, which seeks to provide such a unifying account. Probabilistic semantics extends standard model-theoretic
accounts by assuming a probability distribution over
models. We sketch a probabilistic extension for a Montague-style semantics and consider an application  to the core natural language processing task of textual entailment. An implementation is presented and it is shown that the parameters of a probability distribution over interpretations may be estimated from language data.


% \end{quote}
\end{abstract}


\section{Introduction}

Techniques which represent the meanings of words in terms of the contexts in
which they occur have become an indispensible tool in natural language
processing. Such distributional representations of meaning typically
represent words as vectors or probability distributions over
contexts. They have been applied to a wide variety of tasks,
including sentiment classification~\cite{Bollegala2011}, word sense
disambiguation~\cite{miller-EtAl:2012:PAPERS,khapra-EtAl:2010:ACL}, PP
attachment~\cite{Calvo05distributionalthesaurus}, automatic confusion
set generation~\cite{xue-hwa:2012:PAPERS}, textual
entailment~\cite{berant-dagan-goldberger:2010:ACL}, co-reference
resolution~\cite{lee-EtAl:2012:EMNLP-CoNLL}, predicting semantic
compositionality~\cite{bergsma-EtAl:2010:EMNLP}, acquisition of
semantic lexicons~\cite{mcintosh:2010:EMNLP}, conversation
entailment~\cite{zhang-chai:2010:EMNLP}, semantic role
classification~\cite{zapirain-EtAl:2010:NAACLHLT}, lexical
substitution~\cite{szarvas-biemann-gurevych:2013:NAACL-HLT}, taxonomy
induction~\cite{fountain-lapata:2012:NAACL-HLT}, detection of visual
text~\cite{dodge-EtAl:2012:NAACL-HLT}, and parser
lexicalisation~\cite{rei-briscoe:2013:NAACL-HLT}.

Current distributional approaches to the representation of meaning differ in many respects from compositional, model-theoretic treatments
such as Montague semantics or
Discourse Representation Theory \cite{Blackburn:05}. Whilst distributional approaches provide an account of word meaning, but say little about how those meanings combine, compositional approaches provide an account of the way in which meanings combine to build a semantics for larger phrases, but have little to say about the meanings of words or how they are acquired.  Recently however, a number of researchers have begun to address the problem of  developing a unified account of natural language semantics that combines the strengths of both the distributional and compositional approaches
\cite{Clarke:07,Coecke:10,Garrette:11,Lewis:13}.  In this paper, we present a new formalism, {\em probabilistic semantics\/}, which seeks to provide such a unifying
account. A key idea in probabilistic semantics is to extend standard model-theoretic
accounts of meaning by assuming a probability distribution over
models.  

In model-theoretic semantics, one way to view the meaning
of a logical sentence $s$ is as the set of all interpretations
$\mathcal{M}_s$ for which $s$ is true. Interpretations in $\mathcal{M}_s$ are also called {\em models for\/}
$s$. Then $s$ {\em logically entails\/} $t$ if and only if any model for $s$ is a model for $t$: i.e. $\mathcal{M}_s \subseteq
\mathcal{M}_t$. We propose an extension to this idea where we assume
that models occur randomly. Informally, we may assume a probability
distribution over the set of models. Given such a probability distribution we can then estimate the
probability of a sentence $s$ as the sum of the probabilities of all
models $\mathcal{M}_s$, for $s$. 


Our approach satisfies each of the following desirable properties:
\begin{itemize}
\item It is strongly grounded in probability theory: no ad-hoc methods
  or heuristics are used to combine distributional semantics with
  logical semantics.
\item It provides the basis of an elegant extension of Montague semantics to the
  probabilistic domain: in particular it combines the strengths of a compositional, model-theoretic account of semantics, complete with a well-defined notion of entailment, with a distributional account of the meaning of expressions.
\item It maintains the tradition of logical approaches to semantics
  which are concerned with what models are satisfied rather than
  assuming a single model: this means that composition does not
  necessarily lead to a loss of meaning (for example by reducing the meaning of a
  sentence to a single truth value).
 \item It admits an implementation in which probability
distributions over models can be learnt from data: the account is holistic, in that the learning word meaning and composition are not separated, with
  the potential benefit of more flexible and accurate models of
  compositional distributional semantics.
\end{itemize}


In the following we sketch an approach to constructing a probabilistic extension of Montague semantics and present a way of defining a probability distribution over models in terms of  a random generation process over interpretations.  We consider an application to the core task of textual entailment and describe an implementation of our approach within the symbolic-statistical logic programming framework proposed by Sato \cite{Sato:95}. We show that it is possible to estimate the parameters of a distribution over interpretations from natural language data and relate the learnt distributions to other distributional models of meaning.


% I'm not sure about referring to "distribution semantics" as it may
% be confused with distributional semantics - perhaps just refer to it
% as Sato's framework? Or else note the distinction.

% Yes - I agree with that - removed reference to distribution semantics.


\section{Towards a Probabilistic Montague Semantics}


In a series of papers in the early 1970s \cite{Montague1970a,Montague1970b,Montague1973} the linguist and philosopher Richard Montague spelled out a formal treatment of the semantics of natural language. Montague's conception of semantics was truth-conditional and model-theoretic. He considered that a fundamental aim of any adequate theory of semantics was ``to characterise the notions of a true sentence (under a given interpretation) and of entailment'' \cite{Montague1970b}. A central methodological component of Montague's approach was the {\em Principle of Compositionality\/}, which states that the meaning of an expression is a function of the meanings of its parts and the way they are combined syntactically.


We assume, as in Montague semantics, that natural language expressions
are parsed by a categorial grammar. Further, we assume that with every word there is
associated a function with a type. Let $\mathcal{T}$ be the set of types defined
recursively, such that:
\begin{description}
\item [Basic types:] $e,t\in \mathcal{T}$
\item[Complex types:]  if $\alpha, \beta\in \mathcal{T}$, then $\alpha/\beta\in \mathcal{T}$.
\end{description}
Nothing else is in $\mathcal{T}$. Note that the type $\alpha/\beta$ denotes the type of a function from type
$\alpha$ to type $\beta$.

We define the set $B$ of \emph{basic expressions} to be a set of
symbols denoting meanings of words. We assume that associated with
each $b\in B$ there is a type $\tau_b$. The set $M$ of
\emph{meaningful expressions}, and the extension of $\tau$ to $M$ are
defined recursively such that
\begin{itemize}
\item $B\subseteq M$
\item for every pair $\gamma,\delta\in M$ such that $\tau_\gamma =
  \alpha/\beta$ and $\tau_\delta = \alpha$, then $\gamma(\delta)\in
  M$, and $\tau_{\gamma(\delta)} = \beta$.
\end{itemize}

A \emph{sentence} is a meaningful expression of type $t$. An
\emph{interpretation} is a function $\phi$ which assigns to every
sentence a value of true or false, denoted $\top, \bot$ respectively.

A \emph{theory} is a set $T$ of pairs $(s,\hat{s})$, where $s$ is a
sentence and $\hat{s}\in\{\top,\bot\}$ is a truth value. A
\emph{model} for a theory $T$ is an interpretation which assigns
$\hat{s}$ to every sentence $s\in T$. In this case we say that the model \emph{satisfies} $T$. Let $\Omega$ be the set of all
interpretations, and $\sigma$ be the sigma algebra consisting of the
set of all sets of models that satisfy some theory. Let $\mu$ be a probability measure
on $\sigma$; then $\langle\Omega,\sigma,\mu\rangle$ is a probability
space which describes the probability of theories. Let $M(T)$ denote
the set of all models for the theory $T$; the probability of $T$ is
$\mu(M(T))$.

Given two sentences $s_1$ and $s_2$, we can compute the conditional
probability of $s_1$ given $s_2$ as
$$\frac{\mu(M(\{(s_1, \top), (s_2, \top)\}))}{\mu(M(\{(s_2,\top)\}))}$$
We interpret this conditional probability as the {\em degree to which $s_2$ entails $s_1$\/}. Note that if $s_2$ logical entails $s_1$ then the degree to which  $s_2$ entails $s_1$ is $1.0$.

% In this paper, we consider the situation where the logical language
% $L$ is restricted to that of logic programs, i.e.~Horn clauses with
% universal quantification, and we use Herbrand models. In this case,
% the distribution can be defined in terms of minimal models, as
% demonstrated by Sato (\citeyear{Sato:95}), meaning that probabilities
% of logical sentences can be computed efficiently. He also shows that
% in this case distributions can be efficiently learnt from data using
% the Expectation Maximisation (EM) algorithm. We demonstrate how this
% can work for toy data consisting of natural language sentences.

%\section{Learning}

\section{Probability Distributions over Interpretations}

One can envisage many possible ways of describing probability measures
$\mu$. In this paper we present a simple method in which
interpretations are assumed to be generated by a random process. Suppose that each semantic type is associated with a set of possible values. 
We assume that these values are randomly generated and that their
probabilities are conditionally independent given a hidden
variable. Let $H$ denote the set of possible hidden values and $V(\tau)$  the
set of possible values for type $\tau$. In particular, $V(t) = \{\top, \bot\}$. We assume the following process to generate
interpretations:
\begin{itemize}
\item Generate a hidden value $h\in H$
\item For each sentence $s$:
\begin{itemize}
\item For each basic expression $b$ in $s$, choose a value $v$ from
  $V(\tau_b)$ according to some distribution $P(v|b,h)$.
\item Recursively, for every meaningful expression $\gamma(\delta)$ in
  $s$, where the values of $\delta$ and $\gamma$ have been chosen as
  $v_\delta$ and $v_\gamma$, choose a value $v$ from
  $V(\tau_{\delta(\gamma)})$ according to $P(v|v_\delta, v_\gamma,
  h)$.
\end{itemize}
\end{itemize}

In the following, we show that it is possible to learn the parameters
of this random generation process from data.  Our method falls within
the framework of \citet{Sato:95}. Consequently, we are able to reuse his
algorithms for learning and inference.  Given a set of observed
theories, the probability distributions $P$ can be learnt. In the
following we demonstrate this with examples, and give the code listing
for our implementation. It is also observed that the learnt parameters for a particular basic expression can
be viewed as its distributional representation. In
Section~\ref{sec:recovering} we consider how this representation
relates to standard approaches to distributional semantics.


\section{Application to Textual Entailment}

The task of recognising textual entailment \cite{Dagan:05} is to
determine, given two sentences $T$ (the \emph{text}) and $H$ (the
\emph{hypothesis}), whether $T$ {\em textually entails\/} $H$. It is important to note that the notion of
textual entailment is an informal one based on human judgments. This makes textual entailment much closer to the notion of degree of entailment available within our probabilistic approach than that of strict logical entailment.  The importance of textual entailment within natural language processing lies in its broad relevance to a range of tasks. It represents a core problem, a solution to which is required in order to develop applications in areas such as
information retrieval, machine translation, question answering and
summarisation.

Previous attempts to apply logical representations of meaning to this
task have applied {\em ad hoc\/} heuristics to handle the lack of robustness in
standard approaches. \citet{Bos:06} used model builders to construct a
model for $T$ and a model for $T$ and $H$ together and used the size
of these models as an indication of the degree of entailment. The
intuition here is that if the model for $T$ and $H$ together is larger
than that for just $T$, then $H$ contains a lot of knowledge not in
$T$ and there is a low degree of entailment.  This idea can be
directly related to our approach: it amounts to assuming the existence of a probability
distribution over models,
with larger models less probable than smaller ones.

Textual entailment datasets can also be used to learn the parameters
described in the previous section. A dataset typically consists of a
set of natural language sentence pairs, each annotated as to whether
entailment holds or not. Suppose we are given a pair $(T,H)$ with associated meaningful expressions $s_T$ and $s_H$. In the case that entailment holds for $T$ and $H$ then the dataset will contain the following theories as observations:
\[
\{(s_T,\top),(s_H,\top)\}, \{(s_T,\bot),(s_H,\top)\},\{(s_T,\bot),(s_H,\bot)\}
\]
In the case that entailment does not hold, on the other hand, then the dataset will contain just the following as an observation:
\[
\{(s_T,\top),(s_H,\bot)\}
\]

This
idea could be used as a supervised approach to the general task of recognising textual
entailment.  However, we leave the evaluation of this idea to future
work. Instead, we present here two simple examples that nonetheless demonstrate that our
approach is capable of learning some important features of natural
language semantics.

\begin{figure*}
\centering
\begin{lstlisting}
% Hidden variable
values(hidden, [h0, h1]).

% Types
values(word(noun, Word, Hidden), [n0, n1]).
values(word(verb, Word, Hidden), [v0, v1]).
values(word(det, Word, Hidden), [d0, d1]).
values(function(s, Value1, Value2, Hidden), [t0, t1]).
values(function(np, Value1, Value2, Hidden), [np0, np1]).
values(function(vp, Value1, Value2, Hidden), [vp0, vp1]).

evaluate(w(Word, Type), Hidden, Result) :-
	msw(word(Type, Word, Hidden), Result).
evaluate(f(Type, X, Y), Hidden, Result) :-
	evaluate(X, Hidden, XResult),
	evaluate(Y, Hidden, YResult),
	msw(function(Type, XResult, YResult, Hidden), Result).

theory([], _).
theory([truth(Sentence, Result)|Tail], Hidden) :-
	evaluate(Sentence, Hidden, Result),
	theory(Tail, Hidden).

theory(T) :-
	msw(hidden, Hidden),
	theory(T, Hidden).
\end{lstlisting}
\caption{A PRISM program describing probability distributions over
  natural language models used for our examples.}
\label{figure:program}
\end{figure*}


\section{Implementation}


An implementation for our approach is shown in Figure 
\ref{figure:program}. The code listed in the figure is written in PRISM \cite{Sato:97}, a probabilistic extension of the Prolog logic-progamming language. Note that to simplify the code, complex types of the underlying Montague grammar are referred to by their corresponding natural language
categories. Thus, type $t$ is represented by \texttt{s} (a sentence), type $(e/t)$ by \texttt{vp} (a verb phrase) and type $(e/t)/t$ by \texttt{np} (a noun phrase). 

The PRISM language augments Prolog with probabilistic predicates and declarations. In particular, it incorporates random switches. The predicate \texttt{values} is used to declare a named switch and associate with it a number of possible outcomes while the probabilistic predicate \texttt{msw} allows a random choice to made amongst these outcomes during execution. 

The PRISM program shown in Figure \ref{figure:program} describes how sentences are randomly assigned truth values using random switches. For example, the declaration 
\begin{center}
\texttt{values(word(noun,Word,Hidden),[n0,n1])} 
\end{center}
introduces a switch for nouns having two possible outcomes, \texttt{n0} and \texttt{n1}.
The outcomes are conditioned on the particular choice of noun (\texttt{Word}) and on the choice of hidden variable (\texttt{Hidden}). Similarly, the \texttt{values} predicate is used to declare switches associated with complex types. For example the values associated with a sentence (\texttt{s}) switch are conditioned on the choices of its component parts and a hidden variable.

The probability of a theory is derived by evaluating the truth conditions of the sentences that make it up. Note that probability distributions are defined over models, not theories, so that  to evaluate the
probability of a theory it is necessary to sum over the probabilities of all of its satisfying models. To achieve this,  the predicate \texttt{prob} is used. An
example query returning the probability of the sentence {\em the cat likes the dog\/} is shown in figure \ref{figure:query}. 




\begin{figure}
\begin{lstlisting}
:- prob(theory([
  truth(f(s, f(np, w(the, det),
                   w(cat, noun)),
             f(vp, w(likes, verb),
                   f(np, w(the, det),
                         w(dog, noun)))),
        t1)]), X).
\end{lstlisting}
\caption{PRISM query to evaluate the probability that the sentence
  \emph{the cat likes the dog} is true.}
\label{figure:query}
\end{figure}

\section{Examples}


The PRISM framework is equipped with a proof-theoretic approach to learning so that it is possible to estimate parameters from observed data. A number of different learning algorithms are implemented within the framework. To estimate the
parameters in the following examples we used the standard expectation maximisation algorithm.

Table \ref{table:lexical} demonstrates that our system can learn
simple lexical entailment rules. In this case it is learnt that \emph{cat} entails
\emph{animal} on the basis of a small number of examples. Table \ref{table:quantifiers} demonstrates that our system can learn
something more significant: it can learn that the entailment direction
is reversed with certain quantifiers. Quantifiers such as \emph{some} are
\textbf{monotonically increasing}, so that \emph{some cats} entails
\emph{some animals}. Quantifiers like \emph{no} and \emph{all} are
\textbf{monotonically decreasing}, so that \emph{no animals} entails
\emph{no cats}. Our system is able to determine this from very little
training data.

\begin{table*}
\centering
\begin{tabular}{|l|l|l|}
\hline
Text & Hypothesis & Ent\\
\hline
\emph{the cat chased the dog} & \emph{the animal chased the dog} & \\
\emph{the cat likes the dog} & \emph{the animal likes the dog} & \\
\hline
\emph{the cat loves the dog} & \emph{the animal loves the dog} & 1.0\\
\emph{the animal loves the dog} & \emph{the cat loves the dog} & 0.5\\
\hline
\end{tabular}
\caption{Learning lexical entailment from examples, showing training
  data at the top, and test data below, with the degree of entailment
  determined by our system.}
\label{table:lexical}
\end{table*}

\begin{table*}
\centering
\begin{tabular}{|l|l|l|}
\hline
Text & Hypothesis & Ent\\
\hline
\emph{some cats like all dogs} & \emph{some animals like all dogs} & \\
\emph{no animals like all dogs} & \emph{no cats like all dogs} & \\
\emph{some men like all dogs} & \emph{some people like all dogs} & \\
\hline
\emph{no people like all dogs} & \emph{no men like all dogs} & 1.0\\
\emph{no men like all dogs} & \emph{no people like all dogs} & 0.5\\
\emph{most people like all dogs} & \emph{most men like all dogs} & 0.75\\
\hline
\end{tabular}
\caption{Learning how quantifiers reverse the entailment direction,
  with training data at the top, and test data below, with the degree
  of entailment determined by our system.}
\label{table:quantifiers}
\end{table*}

\begin{table}
\centering
\csvautotabular{lexical.csv}
\caption{Learnt probabilities for the nouns in the example
  in Table \ref{table:quantifiers}}
\label{table:lexical}
\end{table}

\section{Recovering Lexical Semantics}
\label{sec:recovering}

As may be apparent, our formalism does not make use of any explicit representation of
lexical semantics. However, distributional representations of word
meanings can be recovered from the learnt models. The representation
of a word in this case is a matrix, rather than a vector, because there is a different 
probability distribution over the possible values for a particular
type corresponding to each hidden value. For example, Table \ref{table:lexical} shows the learnt
values for the nouns in the example in Table
\ref{table:quantifiers}. Allowing the distribution to vary with the
hidden variable is precisely what enables learning of the behaviour of
quantifiers. Without this flexibility learning would not be possible.

If we want to compare words in a manner similar to vector based
representations of meaning, we can consider the matrix entries as a
flat vector. In this case we can use any of the standard similarity measures (e.g. 
cosine) to compare word representations.

\section{Related Work}

%\cite{Clarke:07,Coecke:10,Garrette:11,Lewis:13}. 

\citet{Coecke:10} proposed a framework based on category-theoretic
similarities between vector spaces and pregroup grammars. Their
approach is arguably the most closely related to ours since it is also
founded in Montague semantics: words are treated as linear functions
between vector spaces. Each word is represented as an element of a
tensor product space, and a pregroup grammar is used to determine how
these are composed using the inner product. It was recently
demonstrated that this approach can be extended to allow the
simulation of predicate calculus using
tensors. \cite{Grefenstette:13}.

There are two main differences between their approach and ours. First,
in the language of Montague semantics, they assume a single
model. This means that composition necessarily loses information about
the composed words, and this is evident in the use of the inner
product for composition. Sentences are all described in a space of
fixed dimensionality, no matter how long or complex they are. In our
approach, a sentence describes a probability distribution over models,
which is an infinite set, allowing for the representation of arbitrary
complexity.

The second difference is that their approach is vector-based whereas
ours is probabilistic in nature. Thus their approach is more
closely related to approaches to lexical distributional semantics
which are vector based, such as latent semantic analysis
\cite{Deerwester:90} and random indexing \cite{Sahlgren:02}, whereas
ours is closer to probabilistic approaches such as latent Dirichlet
allocation \cite{Blei:03}.

\citet{Garrette:11} describe an approach to combining logical
semantics with distributional semantics using Markov Logic Networks
\cite{Richardson:06}. Sentences are parsed into logical form using
Boxer \cite{Bos:04}, and probabilistic rules are added using the
distributional model of \citet{Erk:10}.

\citet{Lewis:13} take a standard logical approach to semantics except
that the relational constants used are derived from distributional
clustering. They use latent dirichlet allocation to describe a
probability distribution over semantic categories for nouns, which are
then combined with induced relation clusters to generate a probability
distribution over logical forms for a given sentence.

\section{Conclusion}

We have introduced probabilistic semantics for natural language, a
framework for representing natural language meaning which extends
Montague semantics by assuming a probability distribution over
models. We have shown how, given some further assumptions, this
distribution can be learnt from data, and discussed the application of
this idea to the task of recognising textual entailment. 

There are a number of directions in which the work may be developed. We are currently  exploring the application of probabilistic semantics to the general task of recognising textual entailment. This would involve learning from much larger datasets and provide a more stringent test of the practical application of the approach. More work is also needed to explore the properties of the distributional representations for words that are learnt by the model. Finally, the implementation presented here deals with a very small fragment of natural language comprised of simple declarative sentences. There are many more complex natural language phenomena that have been dealt with successfully within Montague semantic. This suggests that it should be possible to apply probabilistic semantics to learn about a wide range of phenomena such as quantifier scope ambiguity, intensional contexts, time and tense and indexicals, amongst others. 


\bibliographystyle{acl}
\bibliography{JW2012}
\end{document}
