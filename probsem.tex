%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{color}
\usepackage{listings}

\lstset{
         basicstyle=\footnotesize\ttfamily, % Standardschrift
         %numbers=left,               % Ort der Zeilennummern
         numberstyle=\tiny,          % Stil der Zeilennummern
         %stepnumber=2,               % Abstand zwischen den
         %Zeilennummern
         numbersep=5pt,              % Abstand der Nummern zum Text
         tabsize=2,                  % Groesse von Tabs
         extendedchars=true,         %
         breaklines=true,            % Zeilen werden Umgebrochen
         keywordstyle=\color{black},
         frame=b,         
 %        keywordstyle=[1]\textbf,    % Stil der Keywords
 %        keywordstyle=[2]\textbf,    %
 %        keywordstyle=[3]\textbf,    %
 %        keywordstyle=[4]\textbf,   \sqrt{\sqrt{}} %
         stringstyle=\color{white}\ttfamily, % Farbe der String
         showspaces=false,           % Leerzeichen anzeigen ?
         showtabs=false,             % Tabs anzeigen ?
         xleftmargin=17pt,
         framexleftmargin=17pt,
         framexrightmargin=5pt,
         framexbottommargin=4pt,
         %backgroundcolor=\color{lightgray},
         showstringspaces=false,      % Leerzeichen in Strings anzeigen
         language=prolog
 }

\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (Probabilistic Semantics for Natural Language)
/Author (Daoud Clarke, Bill Keller)}
\setcounter{secnumdepth}{0}  
 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Probabilistic Semantics for Natural Language}
\author{Daoud Clarke and Bill Keller\\
Department of Informatics\\
University of Sussex\\
Falmer, Brighton, UK\\
}
\maketitle
% \begin{abstract}
% \begin{quote}

% \end{quote}
% \end{abstract}


\section{Introduction}

Techniques which represent the meanings of words by the contexts in
which they occur have become an indispensible tool in natural language
processing. Such distributional representations of meaning typically
represent words as vectors or probability distributions over
contexts. They have been applied to a wide variety of tasks,
including sentiment classification~\cite{Bollegala2011}, word sense
disambiguation~\cite{miller-EtAl:2012:PAPERS,khapra-EtAl:2010:ACL}, PP
attachment~\cite{Calvo05distributionalthesaurus}, automatic confusion
set generation~\cite{xue-hwa:2012:PAPERS}, textual
entailment~\cite{berant-dagan-goldberger:2010:ACL}, co-reference
resolution~\cite{lee-EtAl:2012:EMNLP-CoNLL}, predicting semantic
compositionality~\cite{bergsma-EtAl:2010:EMNLP}, acquisition of
semantic lexicons~\cite{mcintosh:2010:EMNLP}, conversation
entailment~\cite{zhang-chai:2010:EMNLP}, semantic role
classification~\cite{zapirain-EtAl:2010:NAACLHLT}, lexical
substitution~\cite{szarvas-biemann-gurevych:2013:NAACL-HLT}, taxonomy
induction~\cite{fountain-lapata:2012:NAACL-HLT}, detection of visual
text~\cite{dodge-EtAl:2012:NAACL-HLT}, and parser
lexicalisation~\cite{rei-briscoe:2013:NAACL-HLT}.

Such approaches to representing meaning differ markedly from logical, compositional accounts of meaning
such as Montague semantics or
Discourse Representation Theory \cite{Blackburn:05}. Whilst distributional approaches provide an account of meaning at the level of words but have little to say about how those meanings combine, logical approaches provide an account of the way in which meanings can be composed to provide a semantics for larger phrases but have little to say about the meanings of individual words. 
Recently, researchers have begun to address the problem of  developing a unified account of natural language semantics that combines the strengths of both the distributional and logical approaches
\cite{Clarke:07,Coecke:10,Garrette:11,Lewis:13}. 

In this paper, we
will present a new formalism, {\em probabilistic semantics\/}, which seeks to provide such a unifying account. Probabilist semantics extends standard model-theoretic accounts of meaning by assuming a probability distribution over models.  In
model-theoretic semantics, one way to view the ``meaning'' of a
logical sentence $s$ is as the set of all interpretations
$\mathcal{M}_s$ for which $s$ is true, also called models for
$s$. Then $s$ entails $t$ if $\mathcal{M}_s \subseteq
\mathcal{M}_t$. We propose an extension to this idea where we assume
that models occur randomly. Informally, we may assume a probability
distribution over the set of models; we can then estimate the
probability of a sentence $s$ as the sum of the probabilities of all
models $\mathcal{M}_s$. We also show how these probability
distributions can be learnt from data, and relate the learnt
distributions to distributional semantics.

Our approach is unique, in that it satisfies all the following:
\begin{itemize}
\item It is strongly grounded in probability theory; no ad-hoc methods
  or heuristics are used to combine distributional semantics with
  logical semantics.
\item It provides an elegant extension of Montague semantics to the
  probabilistic domain.
\item It is holistic, in that the learning of a word's meaning and the
  learning of its composition are not separated. We believe this has
  the potential to lead to more flexible and accurate models of
  compositional distributional semantics.
\item It keeps with the tradition of logical approaches to semantics
  which are concerned with what models are satisfied rather than
  assuming a single model. [POSSIBLE WORLDS?] This means that composition does not
  necessarily lose meaning (for example by reducing the meaning of a
  sentence to a single truth value).
\end{itemize}

[I FEEL THAT THERE OUGHT TO BE SOME KIND OF BRIEF STATEMENT HERE ABOUT WHAT WE WILL SHOW IN THE PAPER, BUT NOT ENTIRELY CLEAR WHAT WE MIGHT SAY YET]

\section{Probabilistic Montague Semantics}

We assume, as in Montague semantics, that natural language expressions
are parsed by a categorial grammar, and that with every word there is
associated a function with a type. The set $\mathcal{T}$ of types is defined
recursively, such that:
\begin{itemize}
\item $e,t\in \mathcal{T}$
\item if $\alpha, \beta\in \mathcal{T}$, then $\alpha/\beta\in \mathcal{T}$.
\end{itemize}
The type $\alpha/\beta$ denotes the type of a function from type
$\alpha$ to type $\beta$.

We define the set $B$ of \emph{basic expressions} to be a set of
symbols denoting meanings of words. We assume that associated with
each $b\in B$ there is a type $\tau_b$. The set $M$ of
\emph{meaningful expressions}, and the extension of $\tau$ to $M$ are
defined recursively such that
\begin{itemize}
\item $B\subseteq M$
\item for every pair $\gamma,\delta\in M$ such that $\tau_\gamma =
  \alpha/\beta$ and $\tau_\delta = \beta$, then $\gamma(\delta)\in
  M$, and $\tau_{\gamma(\delta)} = \alpha$.
\end{itemize}

A \emph{sentence} is a meaningful expression of type $t$. An
\emph{interpretation} is a function $\phi$ which assigns to every
sentence a value of true or false, denoted $\top, \bot$ respectively.

A \emph{theory} is a set $T$ of pairs $(s,\hat{s})$, where $s$ is a
sentence and $\hat{s}\in\{\top,\bot\}$ is a truth value. A
\emph{model} for a theory $T$ is an interpretation which assigns
$\hat{s}$ to every sentence $s\in T$ and in this case we say that the model \emph{satisfies} $T$. Let $\Omega$ be the set of all
interpretations, and $\sigma$ be the sigma algebra consisting of the
set of all sets of models that satisfy some theory. Let $\mu$ be a probability measure
on $\sigma$; then $\langle\Omega,\sigma,\mu\rangle$ is a probability
space which describes the probability of theories. Let $M(T)$ denote
the set of all models for the theory $T$; the probability of $T$ is
$\mu(M(T))$.

Given two sentences $s_1$ and $s_2$, we can compute the conditional
probability of $s_1$ given $s_2$ as
$$\frac{\mu(M(\{(s_1, \top), (s_2, \top)\}))}{\mu(M(\{(s_2,\top)\}))}$$
We interpret this as the degree to which $s_2$ entails $s_1$.

% In this paper, we consider the situation where the logical language
% $L$ is restricted to that of logic programs, i.e.~Horn clauses with
% universal quantification, and we use Herbrand models. In this case,
% the distribution can be defined in terms of minimal models, as
% demonstrated by Sato (\citeyear{Sato:95}), meaning that probabilities
% of logical sentences can be computed efficiently. He also shows that
% in this case distributions can be efficiently learnt from data using
% the Expectation Maximisation (EM) algorithm. We demonstrate how this
% can work for toy data consisting of natural language sentences.

\section{Learning}

One can envisage many possible ways of describing probability measures
$\mu$. In this paper we will present a simple method in which
interpretations are assumed to be generated by a random process. We
will show that it is possible to learn the parameters of the random
generation process from data. Our method falls within the framework of
Sato (\citeyear{Sato:95}), meaning that we can reuse his algorithms
for learning and inference.

To make the number of parameters small enough to make learning
feasible, we restrict the number of possible values of each semantic
type. We assume that these values are randomly generated, and their
probabilities are conditionally independent given a hidden
variable. Specifically, we assume the following process to generate
interpretations:
\begin{itemize}
\item Generate a hidden value $h\in H$
\item For each sentence $s$:
\begin{itemize}
\item For each basic expression $b$ in $s$, choose a value $v$ from
  $V(\tau_b)$ according to some distribution $P(v|b,h)$.
\item Recursively, for every meaningful expression $\gamma(\delta)$ in
  $s$, where the values of $\delta$ and $\gamma$ have been chosen as
  $v_\delta$ and $v_\gamma$, choose a value $v$ from
  $V(\tau_{\delta(\gamma)})$ according to $P(v|v_\delta, v_\gamma,
  h)$.
\end{itemize}
\end{itemize}
where $H$ is the set of possible hidden values, and $V(\tau)$ is the
set of possible values for type $\tau$, with $V(t) = \{\top, \bot\}$.

Given a set of observed theories, the probability distributions $P$
can be learnt. In the full paper, we will demonstrate this with
examples, and give the code listing for our implementation within
Sato's framework.

\section{Application to Textual Entailment}

The task of recognising textual entailment \cite{Dagan:05} is to
determine, given two sentences $T$ (the \emph{text}) and $H$ (the
\emph{hypothesis}), whether $T$ entails or implies $H$. The notion of
entailment is an informal one, and is based on human judgments rather
than requiring strict logical entailment, making it well suited to our
probabilistic approach.

The task has broad relevance within natural language processing, and
is intended to represent a core problem that is required to solve
information retrieval, machine translation, question answering and
summarisation.

% Our 

% \begin{figure}
% \begin{lstlisting}
% % Hidden variable
% values(hidden, [h0, h1, h2]).

% % Types
% values(word(noun, Word, Hidden), [n0, n1, n2, n3, n4, n5]).
% values(word(verb, Word, Hidden), [v0, v1, v2]).
% values(word(det, Word, Hidden), [d0, d1, d2]).
% values(function(s, Value1, Value2, Hidden), [t0, t1]).
% values(function(np, Value1, Value2, Hidden), [np0, np1, np2]).
% values(function(vp, Value1, Value2, Hidden), [vp0, vp1, vp2]).

% evaluate(w(Word, Type), Hidden, Result) :-
% 	msw(word(Type, Word, Hidden), Result).
% evaluate(f(Type, X, Y), Hidden, Result) :-
% 	evaluate(X, Hidden, XResult),
% 	evaluate(Y, Hidden, YResult),
% 	msw(function(Type, XResult, YResult, Hidden), Result).

% theory([], _).
% theory([truth(Sentence, Result)|Tail], Hidden) :-
% 	evaluate(Sentence, Hidden, Result).

% theory(T) :-
% 	msw(hidden, Hidden),
% 	theory(T, Hidden).
% \end{lstlisting}
% \caption{A PRISM program to learn probability distributions over
%   natural language models.}
% \end{figure}


\bibliographystyle{aaai}
\bibliography{JW2012}
\end{document}
