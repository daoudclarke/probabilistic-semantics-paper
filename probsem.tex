%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{color}
\usepackage{listings}

\lstset{
         basicstyle=\footnotesize\ttfamily, % Standardschrift
         %numbers=left,               % Ort der Zeilennummern
         numberstyle=\tiny,          % Stil der Zeilennummern
         %stepnumber=2,               % Abstand zwischen den
         %Zeilennummern
         numbersep=5pt,              % Abstand der Nummern zum Text
         tabsize=2,                  % Groesse von Tabs
         extendedchars=true,         %
         breaklines=true,            % Zeilen werden Umgebrochen
         keywordstyle=\color{black},
         frame=b,         
 %        keywordstyle=[1]\textbf,    % Stil der Keywords
 %        keywordstyle=[2]\textbf,    %
 %        keywordstyle=[3]\textbf,    %
 %        keywordstyle=[4]\textbf,   \sqrt{\sqrt{}} %
         stringstyle=\color{white}\ttfamily, % Farbe der String
         showspaces=false,           % Leerzeichen anzeigen ?
         showtabs=false,             % Tabs anzeigen ?
         xleftmargin=17pt,
         framexleftmargin=17pt,
         framexrightmargin=5pt,
         framexbottommargin=4pt,
         %backgroundcolor=\color{lightgray},
         showstringspaces=false,      % Leerzeichen in Strings anzeigen
         language=prolog
 }

\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (Probabilistic Semantics for Natural Language)
/Author (Daoud Clarke, Bill Keller)}
\setcounter{secnumdepth}{0}  
 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Probabilistic Semantics for Natural Language}
\author{Daoud Clarke and Bill Keller\\
Department of Informatics\\
University of Sussex\\
Falmer, Brighton, UK\\
}
\maketitle
% \begin{abstract}
% \begin{quote}

% \end{quote}
% \end{abstract}


\section{Introduction}

Techniques which represent the meanings of words by the contexts in
which they occur have become an indispensible tool in natural language
processing. Such distributional representations of meaning typically
represent words as vectors or probability distributions over
contexts. They have been applied to a wide variety of tasks,
including sentiment classification~\cite{Bollegala2011}, word sense
disambiguation~\cite{miller-EtAl:2012:PAPERS,khapra-EtAl:2010:ACL}, PP
attachment~\cite{Calvo05distributionalthesaurus}, automatic confusion
set generation~\cite{xue-hwa:2012:PAPERS}, textual
entailment~\cite{berant-dagan-goldberger:2010:ACL}, co-reference
resolution~\cite{lee-EtAl:2012:EMNLP-CoNLL}, predicting semantic
compositionality~\cite{bergsma-EtAl:2010:EMNLP}, acquisition of
semantic lexicons~\cite{mcintosh:2010:EMNLP}, conversation
entailment~\cite{zhang-chai:2010:EMNLP}, semantic role
classification~\cite{zapirain-EtAl:2010:NAACLHLT}, lexical
substitution~\cite{szarvas-biemann-gurevych:2013:NAACL-HLT}, taxonomy
induction~\cite{fountain-lapata:2012:NAACL-HLT}, detection of visual
text~\cite{dodge-EtAl:2012:NAACL-HLT}, and parser
lexicalisation~\cite{rei-briscoe:2013:NAACL-HLT}.

Such representations of meaning seem to be in conflict with logical
approaches to representing meaning, such as Montague semantics or
Discourse Representation Theory \cite{Blackburn:05}. In these
approaches, a sentence is translated to a logical form, which is then
used for reasoning via theorem proving or model building.

Recently, effort has been focussed on resolving this conflict
\cite{Clarke:07,Coecke:10,Garrette:11,Lewis:13}. In this paper, we
will present a new formalism, probabilistic semantics. In standard
model-theoretic semantics, one way to view the ``meaning'' of a
logical sentence $s$ is as the set of all interpretations
$\mathcal{M}_s$ for which $s$ is true, also called models for
$s$. Then $s$ entails $t$ if $\mathcal{M}_s \subseteq
\mathcal{M}_t$. We propose an extension to this idea where we assume
that models occur randomly. Informally, we may assume a probability
distribution over the set of models; we can then estimate the
probability of a sentence $s$ as the sum of the probabilities of all
models $\mathcal{M}_s$. We also show how these probability
distributions can be learnt from data, and relate the learnt
distributions to distributional semantics.

Our approach is unique, in that it satisfies all the following:
\begin{itemize}
\item It is strongly grounded in probability theory; no ad-hoc methods
  or heuristics are used to combine distributional semantics with
  logical semantics.
\item It provides an elegant extension of Montague semantics to the
  probabilistic domain.
\item It is holistic, in that the learning of a word's meaning and the
  learning of its composition are not separated. We believe this has
  the potential to lead to more flexible and accurate models of
  compositional distributional semantics.
\item It keeps with the tradition of logical approaches to semantics
  which are concerned with what models are satisfied rather than
  assuming a single model. This means that composition does not
  necessarily lose meaning (for example by reducing the meaning of a
  sentence to a single truth value).
\end{itemize}


\section{Probabilistic Montague Semantics}

We assume, as in Montague semantics, that natural language expressions
are parsed by a categorial grammar, and that with every word there is
associated a function with a type. The set $T$ of types is defined
recursively, such that:
\begin{itemize}
\item $e,t\in T$
\item if $\alpha, \beta\in T$, then $\alpha/\beta\in T$.
\end{itemize}
The type $\alpha/\beta$ denotes the type of a function from type
$\alpha$ to type $\beta$.

We define the set $B$ of \emph{basic expressions} to be a set of
symbols denoting meanings of words. We assume that associated with
each $b\in B$ there is a type $\tau_b$. The set $M$ of
\emph{meaningful expressions}, and the extension of $\tau$ to $M$ are
defined recursively such that
\begin{itemize}
\item $B\subseteq M$
\item for every pair $\gamma,\delta\in M$ such that $\tau_\gamma =
  \alpha/\beta$ and $\tau_\delta = \beta$, then $\gamma(\delta)\in
  M$, and $\tau_{\gamma(\delta)} = \alpha$.
\end{itemize}

A \emph{sentence} is a meaningful expression of type $t$. An
\emph{interpretation} is a function $\phi$ which assigns to every
sentence a value of true or false, denoted $\top, \bot$ respectively.

A \emph{theory} is a set $T$ of pairs $(s,\hat{s})$, where $s$ is a
sentence and $\hat{s}\in\{\top,\bot\}$ is a truth value. A
\emph{model} for a theory $T$ is an interpretation which assigns
$\hat{s}$ to every sentence $s\in T$. Let $\Omega$ be the set of all
interpretations, and $\sigma$ be the sigma algebra consisting of the
set of all models for all theories. Let $\mu$ be a probability measure
on $\sigma$; then $\langle\Omega,\sigma,\mu\rangle$ is a probability
space which describes the probability of theories. Let $M(T)$ denote
the set of all models for the theory $T$; the probability of $T$ is
$\mu(M(T))$.

Given two sentences $s_1$ and $s_2$, we can compute the conditional
probability of $s_1$ given $s_2$ as
$$\frac{\mu(M(\{(s_1, \top), (s_2, \top)\}))}{\mu(M(\{(s_2,\top)\}))}$$
We interpret this as the degree to which $s_2$ entails $s_1$.

% In this paper, we consider the situation where the logical language
% $L$ is restricted to that of logic programs, i.e.~Horn clauses with
% universal quantification, and we use Herbrand models. In this case,
% the distribution can be defined in terms of minimal models, as
% demonstrated by Sato (\citeyear{Sato:95}), meaning that probabilities
% of logical sentences can be computed efficiently. He also shows that
% in this case distributions can be efficiently learnt from data using
% the Expectation Maximisation (EM) algorithm. We demonstrate how this
% can work for toy data consisting of natural language sentences.

\section{Learning}

One can envisage many possible ways of describing probability measures
$\mu$. In this paper we will present a simple method in which
interpretations are assumed to be generated by a random process. We
will show that it is possible to learn the parameters of the random
generation process from data. Our method falls within the framework of
Sato (\citeyear{Sato:95}), meaning that we can reuse his algorithms
for learning parameters from data, and computing probabilities of
theories.

\begin{figure}
\begin{lstlisting}
% Hidden variable
values(hidden, [h0, h1, h2, h3, h4]).

% Probabilities conditional on hidden variable
values(subject(_,_), [true, false]).
values(verb(_,_), [true, false]).
values(object(_,_), [true, false]).

sentence_truth(S, V, O, T) :-
	(S = true, V = true, O = true) -> T = true ; T = false.

all_true([], _).
all_true([[Subject, Verb, Object, Truth]|XT], Hidden) :-
	msw(subject(Subject, Hidden), ST),
	msw(verb(Verb, Hidden), VT),
	msw(object(Object, Hidden), OT),
	sentence_truth(ST, VT, OT, Truth),
	all_true(XT, Hidden).

is_true(X) :-
	msw(hidden, Hidden),
	all_true(X, Hidden).

:- learn([count(is_true([[cats, chase, dogs, true],[cats, chase, mice, true],[dogs, chase, cats, true],[dogs,chase,dogs,false]]), 1),
	count(is_true([[cats, eat, cheese, true],[cats, eat, cats, false]]), 1),
	count(is_true([[mice, eat, cheese, true],[cheese, eat, cheese, false]]), 1),
	count(is_true([[mice, eat, cats, false],[mice, chase, cheese, false]]), 1),
	count(is_true([[dogs, eat, cats, false],[dogs, chase, dogs, true]]), 1)]).
\end{lstlisting}
\caption{A PRISM program to learn probability distributions over
  natural language models.}
\end{figure}


\bibliographystyle{aaai}
\bibliography{JW2012}
\end{document}
