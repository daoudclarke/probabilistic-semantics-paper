%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai}
\nocopyright
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{color}
\usepackage{listings}
%\usepackage{natbib}

%\renewcommand{\cite}{\citep}
\newcommand{\citet}[1]{\citeauthor{#1} \shortcite{#1}}

\lstset{
         basicstyle=\footnotesize\ttfamily, % Standardschrift
         %numbers=left,               % Ort der Zeilennummern
         numberstyle=\tiny,          % Stil der Zeilennummern
         %stepnumber=2,               % Abstand zwischen den
         %Zeilennummern
         numbersep=5pt,              % Abstand der Nummern zum Text
         tabsize=2,                  % Groesse von Tabs
         extendedchars=true,         %
         breaklines=true,            % Zeilen werden Umgebrochen
         keywordstyle=\color{black},
         frame=b,         
 %        keywordstyle=[1]\textbf,    % Stil der Keywords
 %        keywordstyle=[2]\textbf,    %
 %        keywordstyle=[3]\textbf,    %
 %        keywordstyle=[4]\textbf,   \sqrt{\sqrt{}} %
         stringstyle=\color{white}\ttfamily, % Farbe der String
         showspaces=false,           % Leerzeichen anzeigen ?
         showtabs=false,             % Tabs anzeigen ?
         xleftmargin=17pt,
         framexleftmargin=17pt,
         framexrightmargin=5pt,
         framexbottommargin=4pt,
         %backgroundcolor=\color{lightgray},
         showstringspaces=false,      % Leerzeichen in Strings anzeigen
         language=prolog
 }

\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (Probabilistic Semantics for Natural Language)
/Author (Daoud Clarke, Bill Keller)}
\setcounter{secnumdepth}{0}  
 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Probabilistic Semantics for Natural Language}
\author{Daoud Clarke and Bill Keller\\
Department of Informatics\\
University of Sussex\\
Falmer, Brighton, UK\\
}
\maketitle
\begin{abstract}
% \begin{quote}
Distributional representations of meaning typically
represent words as vectors or probability distributions over
contexts. Such approaches to the representation of meaning differ markedly from compositional treatments of meaning
such as Montague semantics.Recently, researchers have begun to address the problem of  developing a unified account of natural language semantics that combines the strengths of both the distributional and compositional approaches. This paper presents a new formalism, {\em probabilistic semantics\/}, which seeks to provide such a unifying account. Probabilistic semantics extends standard model-theoretic
accounts of meaning by assuming a probability distribution over
models. We sketch a probabilistic extension of Montague semantics and consider an application of our approach to the core task of textual entailment......


% \end{quote}
\end{abstract}


\section{Introduction}

Techniques which represent the meanings of words in terms of the contexts in
which they occur have become an indispensible tool in natural language
processing. Such distributional representations of meaning typically
represent words as vectors or probability distributions over
contexts. They have been applied to a wide variety of tasks,
including sentiment classification~\cite{Bollegala2011}, word sense
disambiguation~\cite{miller-EtAl:2012:PAPERS,khapra-EtAl:2010:ACL}, PP
attachment~\cite{Calvo05distributionalthesaurus}, automatic confusion
set generation~\cite{xue-hwa:2012:PAPERS}, textual
entailment~\cite{berant-dagan-goldberger:2010:ACL}, co-reference
resolution~\cite{lee-EtAl:2012:EMNLP-CoNLL}, predicting semantic
compositionality~\cite{bergsma-EtAl:2010:EMNLP}, acquisition of
semantic lexicons~\cite{mcintosh:2010:EMNLP}, conversation
entailment~\cite{zhang-chai:2010:EMNLP}, semantic role
classification~\cite{zapirain-EtAl:2010:NAACLHLT}, lexical
substitution~\cite{szarvas-biemann-gurevych:2013:NAACL-HLT}, taxonomy
induction~\cite{fountain-lapata:2012:NAACL-HLT}, detection of visual
text~\cite{dodge-EtAl:2012:NAACL-HLT}, and parser
lexicalisation~\cite{rei-briscoe:2013:NAACL-HLT}.

Current distributional approaches to the representation of meaning differ in many respects from logical, compositional treatments of meaning
such as Montague semantics or
Discourse Representation Theory \cite{Blackburn:05}. Whilst distributional approaches provide an account of meaning at the level of words but have little to say about how those meanings combine, logical approaches provide an account of the way in which meanings can be composed to provide a semantics for larger phrases but have little to say about the meanings of individual words.  Recently however, a number of researchers have begun to address the problem of  developing a unified account of natural language semantics that combines the strengths of both the distributional and logical approaches
\cite{Clarke:07,Coecke:10,Garrette:11,Lewis:13}.  In this paper, we present a new formalism, {\em probabilistic semantics\/}, which seeks to provide such a unifying
account. Probabilistic semantics extends standard model-theoretic
accounts of meaning by assuming a probability distribution over
models.  

In model-theoretic semantics, one way to view the meaning
of a logical sentence $s$ is as the set of all interpretations
$\mathcal{M}_s$ for which $s$ is true. Interpretations in $\mathcal{M}_s$ are also called {\em models for\/}
$s$. Then $s$ entails $t$ if any model for $s$ is a model for $t$: $\mathcal{M}_s \subseteq
\mathcal{M}_t$. We propose an extension to this idea where we assume
that models occur randomly. Informally, we may assume a probability
distribution over the set of models. Given such a probability distribution we can then estimate the
probability of a sentence $s$ as the sum of the probabilities of all
models for $s$, $\mathcal{M}_s$. In the following, we also show how these probability
distributions can be learnt from data and relate the learnt
distributions to other distributional models of semantics.

Our approach is unique, in that it satisfies all the following:
\begin{itemize}
\item It is strongly grounded in probability theory: no ad-hoc methods
  or heuristics are used to combine distributional semantics with
  logical semantics.
\item It provides the basis of an elegant extension of Montague semantics to the
  probabilistic domain: in particular it combines the strengths of a compositional, model-theoretic account of semantics, complete with a well-defined notion of entailment, with a distributional account of the meaning of expressions.
\item It maintains the tradition of logical approaches to semantics
  which are concerned with what models are satisfied rather than
  assuming a single model: this means that composition does not
  necessarily lead to a loss of meaning (for example by reducing the meaning of a
  sentence to a single truth value).
  \item It is holistic, in that the learning of a word's meaning and the
  learning of its composition are not separated. We believe this has
  the potential to lead to more flexible and accurate models of
  compositional distributional semantics.
\end{itemize}

In the following sketch a probabilistic extension of Montague semantics and present a way of defining a probability distribution over models in terms of  a random generation process over interpretations.  We consider an application to the core task of textual entailment and present an implementation of our approach within the framework of distribution semantics \cite{ref}. We show that it is possible to learn ....

\section{Probabilistic Montague Semantics}

[SAY SOMETHING BRIEF AND INTRODUCTORY HERE ABOUT MONTAGUE GRAMMAR/SEMANTICS]

We assume, as in Montague semantics, that natural language expressions
are parsed by a categorial grammar. Further, we assume that with every word there is
associated a function with a type. Let $\mathcal{T}$ be the set of types defined
recursively, such that:
\begin{description}
\item [Basic types:] $e,t\in \mathcal{T}$
\item[Complex types:]  if $\alpha, \beta\in \mathcal{T}$, then $\alpha/\beta\in \mathcal{T}$.
\end{description}
Nothing else is in $\mathcal{T}$. Note that the type $\alpha/\beta$ denotes the type of a function from type
$\alpha$ to type $\beta$.

We define the set $B$ of \emph{basic expressions} to be a set of
symbols denoting meanings of words. We assume that associated with
each $b\in B$ there is a type $\tau_b$. The set $M$ of
\emph{meaningful expressions}, and the extension of $\tau$ to $M$ are
defined recursively such that
\begin{itemize}
\item $B\subseteq M$
\item for every pair $\gamma,\delta\in M$ such that $\tau_\gamma =
  \alpha/\beta$ and $\tau_\delta = \alpha$, then $\gamma(\delta)\in
  M$, and $\tau_{\gamma(\delta)} = \beta$.
\end{itemize}

A \emph{sentence} is a meaningful expression of type $t$. An
\emph{interpretation} is a function $\phi$ which assigns to every
sentence a value of true or false, denoted $\top, \bot$ respectively.

A \emph{theory} is a set $T$ of pairs $(s,\hat{s})$, where $s$ is a
sentence and $\hat{s}\in\{\top,\bot\}$ is a truth value. A
\emph{model} for a theory $T$ is an interpretation which assigns
$\hat{s}$ to every sentence $s\in T$. In this case we say that the model \emph{satisfies} $T$. Let $\Omega$ be the set of all
interpretations, and $\sigma$ be the sigma algebra consisting of the
set of all sets of models that satisfy some theory. Let $\mu$ be a probability measure
on $\sigma$; then $\langle\Omega,\sigma,\mu\rangle$ is a probability
space which describes the probability of theories. Let $M(T)$ denote
the set of all models for the theory $T$; the probability of $T$ is
$\mu(M(T))$.

Given two sentences $s_1$ and $s_2$, we can compute the conditional
probability of $s_1$ given $s_2$ as
$$\frac{\mu(M(\{(s_1, \top), (s_2, \top)\}))}{\mu(M(\{(s_2,\top)\}))}$$
We interpret this as the degree to which $s_2$ entails $s_1$.

% In this paper, we consider the situation where the logical language
% $L$ is restricted to that of logic programs, i.e.~Horn clauses with
% universal quantification, and we use Herbrand models. In this case,
% the distribution can be defined in terms of minimal models, as
% demonstrated by Sato (\citeyear{Sato:95}), meaning that probabilities
% of logical sentences can be computed efficiently. He also shows that
% in this case distributions can be efficiently learnt from data using
% the Expectation Maximisation (EM) algorithm. We demonstrate how this
% can work for toy data consisting of natural language sentences.

%\section{Learning}

\section{Probability Distributions over Interpretations}

One can envisage many possible ways of describing probability measures
$\mu$. In this paper we will present a simple method in which
interpretations are assumed to be generated by a random process. Suppose that each semantic type is associated with a set of possible values. 
We assume that these values are randomly generated and that their
probabilities are conditionally independent given a hidden
variable. Let $H$ denote the set of possible hidden values and $V(\tau)$  the
set of possible values for type $\tau$. In particular, $V(t) = \{\top, \bot\}$. We assume the following process to generate
interpretations:
\begin{itemize}
\item Generate a hidden value $h\in H$
\item For each sentence $s$:
\begin{itemize}
\item For each basic expression $b$ in $s$, choose a value $v$ from
  $V(\tau_b)$ according to some distribution $P(v|b,h)$.
\item Recursively, for every meaningful expression $\gamma(\delta)$ in
  $s$, where the values of $\delta$ and $\gamma$ have been chosen as
  $v_\delta$ and $v_\gamma$, choose a value $v$ from
  $V(\tau_{\delta(\gamma)})$ according to $P(v|v_\delta, v_\gamma,
  h)$.
\end{itemize}
\end{itemize}

In the following, we
show that it is possible to learn the parameters of this random
generation process from data.  Our method falls within the framework of
\citet{Sato:95}, meaning that we can reuse his algorithms
for learning and inference.
Given a set of observed theories, the probability distributions $P$
can be learnt. In the following we demonstrate this with
examples, and give the code listing for our implementation within the framework of Sato's distribution of semantics. The learnt parameters for a particular basic
expression can be viewed as its distributional semantics. In section ??? we consider how this representation relates to standard
approaches to distributional semantics.



To keep the number of parameters small enough to make learning
feasible, we restrict the number of possible values of each semantic
type.

\section{Application to Textual Entailment}

The task of recognising textual entailment \cite{Dagan:05} is to
determine, given two sentences $T$ (the \emph{text}) and $H$ (the
\emph{hypothesis}), whether $T$ entails or implies $H$. The notion of
entailment is an informal one, and is based on human judgments rather
than requiring strict logical entailment, making it well suited to our
probabilistic approach. The task has broad relevance within natural language processing. 
It represents a core problem, a solution to which is required in order to develop applications such as
information retrieval, machine translation, question answering and
summarisation.

Previous attempts to apply logical representations of meaning to this
task have applied heuristics to handle the lack of robustness in
standard approaches. \citet{Bos:06} used model builders to construct a
model for $T$ and a model for $T$ and $H$ together and used the size
of these models as an indication of the degree of entailment. The
intuition here is that if the model for $T$ and $H$ together is larger
than that for just $T$, then $H$ contains a lot of knowledge not in
$T$ and there is a low degree of entailment.  This idea can be
directly related to our approach if we assume a probability
distribution over models which depends only on the size of the model,
making larger models less likely.

Textual entailment datasets can also be used to learn the parameters
described in the previous section. A dataset typically consists of a
set of natural language sentence pairs, each annotated as to whether
entailment holds or not. Given a pair $(T,H)$ for which entailment
does hold, and meaningful expressions $s_T$ and $s_H$ for $T$ and $H$,
then we can add the following theories as observations:
$\{(s_T,\top),(s_H,\top)\}$, $\{(s_T,\bot),(s_H,\top)\}$,
$\{(s_T,\bot),(s_H,\bot)\}$. For a pair for which entailment does not
hold, we can add $\{(s_T,\top),(s_H,\bot)\}$ as an observation. This
idea could potentially be used as a supervised approach to textual
entailment, however we leave the evaluation of this idea to future
work. Instead, we will give some examples to demonstrate that our
model is capable of learning some important features of natural
language semantics.

\section{Implementation}

\begin{figure*}
\centering
\begin{lstlisting}
% Hidden variable
values(hidden, [h0, h1]).

% Types
values(word(noun, Word, Hidden), [n0, n1]).
values(word(verb, Word, Hidden), [v0, v1]).
values(word(det, Word, Hidden), [d0, d1]).
values(function(s, Value1, Value2, Hidden), [t0, t1]).
values(function(np, Value1, Value2, Hidden), [np0, np1]).
values(function(vp, Value1, Value2, Hidden), [vp0, vp1]).

evaluate(w(Word, Type), Hidden, Result) :-
	msw(word(Type, Word, Hidden), Result).
evaluate(f(Type, X, Y), Hidden, Result) :-
	evaluate(X, Hidden, XResult),
	evaluate(Y, Hidden, YResult),
	msw(function(Type, XResult, YResult, Hidden), Result).

theory([], _).
theory([truth(Sentence, Result)|Tail], Hidden) :-
	evaluate(Sentence, Hidden, Result),
	theory(Tail, Hidden).

theory(T) :-
	msw(hidden, Hidden),
	theory(T, Hidden).
\end{lstlisting}
\caption{A PRISM program describing probability distributions over
  natural language models used for our examples.}
\label{figure:program}
\end{figure*}

Figure \ref{figure:program} gives the listing of the program that we
used within the PRISM framework \cite{Sato:97}. It describes how
sentences are randomly assigned truth values, using switches
identified by the special predicate \texttt{msw}. The predicate
\texttt{values} describes the allowed values of each switch, and the
variables on which they are conditioned. To simplify the code, complex
types are referred to by their corresponding natural language
categories, so type $t$ is represented by \texttt{s} (a sentence) and
type $(e/t)/t$ by \texttt{np} (a noun phrase). To evaluate the
probability of a theory, the predicate \texttt{prob} is used; an
example query is shown in figure \ref{figure:query}. To learn the
parameters, we used the standard expectation maximisation algorithm
implemented in PRISM.

\begin{figure}
\begin{lstlisting}
:- prob(theory([
  truth(f(s, f(np, w(the, det),
                   w(cat, noun)),
             f(vp, w(likes, verb),
                   f(np, w(the, det),
                         w(dog, noun)))),
        t1)]), X).
\end{lstlisting}
\caption{PRISM query to evaluate the probability that the sentence
  \emph{the cat likes the dog} is true.}
\label{figure:query}
\end{figure}

\section{Examples}

Table \ref{table:lexical} demonstrates that our system can learn
simple lexical entailment rules, in this case that \emph{cat} entails
\emph{animal}.

Table \ref{table:quantifiers} demonstrates that our system can learn
something more significant: it can learn that the entailment direction
is reversed with certain quantifiers. Quantifiers like \emph{some} are
\textbf{monotonically increasing}, so that \emph{some cats} entails
\emph{some animals}. Quantifiers like \emph{no} and \emph{all} are
\textbf{monotonically decreasing}, so that \emph{no animals} entails
\emph{no cats}. Our system is able to determine this from very little
training data.

\begin{table*}
\centering
\begin{tabular}{|l|l|l|}
\hline
Text & Hypothesis & Ent\\
\hline
\emph{the cat chased the dog} & \emph{the animal chased the dog} & \\
\emph{the cat likes the dog} & \emph{the animal likes the dog} & \\
\hline
\emph{the cat loves the dog} & \emph{the animal loves the dog} & 1.0\\
\emph{the animal loves the dog} & \emph{the cat loves the dog} & 0.5\\
\hline
\end{tabular}
\caption{Learning lexical entailment from examples, showing training
  data at the top, and test data below, with the degree of entailment
  determined by our system.}
\label{table:lexical}
\end{table*}

\begin{table*}
\centering
\begin{tabular}{|l|l|l|}
\hline
Text & Hypothesis & Ent\\
\hline
\emph{some cats like all dogs} & \emph{some animals like all dogs} & \\
\emph{no animals like all dogs} & \emph{no cats like all dogs} & \\
\emph{some men like all dogs} & \emph{some people like all dogs} & \\
\hline
\emph{no people like all dogs} & \emph{no men like all dogs} & 1.0\\
\emph{no men like all dogs} & \emph{no people like all dogs} & 0.5\\
\emph{most people like all dogs} & \emph{most men like all dogs} & 0.75\\
\hline
\end{tabular}
\caption{Learning how quantifiers reverse the entailment direction,
  with training data at the top, and test data below, with the degree
  of entailment determined by our system.}
\label{table:quantifiers}
\end{table*}

\section{Related Work}

%\cite{Clarke:07,Coecke:10,Garrette:11,Lewis:13}. 

\citet{Coecke:10} proposed a framework based on category-theoretic
similarities between vector spaces and pregroup grammars. Their
approach is arguably the most closely related to ours since it is also
founded in Montague semantics: words are treated as linear functions
between vector spaces. Each word is represented as an element of a
tensor product space, and a pregroup grammar is used to determine how
these are composed using the inner product. It was recently
demonstrated that this approach can be extended to allow the
simulation of predicate calculus using
tensors. \cite{Grefenstette:13}.

There are two main differences between their approach and ours: First,
in the language of Montague semantics, they assume a single
model. This means that composition necessarily loses information about
the composed words, and this is evident in the use of the inner
product for composition. Sentences are all described in a space of
fixed dimensionality, no matter how long or complex they are. In our
approach, a sentence describes a probability distribution over models,
which is an infinite set, allowing for the representation of arbitrary
complexity.

The second difference is that their approach is vector-based whereas
ours is probabilistic in nature. Thus their approach is thus more
closely related to approaches to lexical distributional semantics
which are vector based, such as latent semantic analysis
\cite{Deerwester:90} and random indexing \cite{Sahlgren:02}, whereas
ours is closer to probabilistic approaches such as latent Dirichlet
allocation \cite{Blei:03}.

\citet{Garrette:11} describe an approach to combining logical
semantics with distributional semantics using Markov Logic Networks
\cite{Richardson:06}. Sentences are parsed into logical form using
Boxer \cite{Bos:04}, and probabilistic rules are added using the
distributional model of 



\section{Conclusion}

We have introduced probabilistic semantics for natural language, a
framework for representing natural language meaning which extends
Montague semantics by assuming a probability distribution over
models. We have shown how, given some further assumptions, this
distribution can be learnt from data, and discussed the application of
this idea to the task of recognising textual entailment.


\bibliographystyle{aaai}
\bibliography{JW2012}
\end{document}
