\documentclass[11pt]{article}
\usepackage{acl2012}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}
\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

%\documentclass{svmult}

% % SVMult required packages
% \usepackage{mathptmx}
% \usepackage{helvet}
% \usepackage{courier}
% \usepackage{graphicx}
% \usepackage{makeidx}
% \usepackage{multicol}
% \usepackage{footmisc}


%\usepackage{url}
%\usepackage{latexsym}

\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{color}
\usepackage{listings}
\usepackage{csvsimple}
\usepackage{graphicx}
%\usepackage{caption}
%\usepackage{subcaption}

%\renewcommand{\theTitleReference}[2]{\emph{#2}}

%\renewcommand{\cite}{\citep}
%\newcommand{\citet}[1]{\citeauthor{#1} \shortcite{#1}}
%\newcommand{\citet}[1]{\cite{#1}}

\newtheorem*{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem*{definition}{Definition}

\lstset{
  %basicstyle=\footnotesize\ttfamily, % Standardschrift
         basicstyle=\scriptsize\ttfamily, % Standardschrift
         %numbers=left,               % Ort der Zeilennummern
         numberstyle=\tiny,          % Stil der Zeilennummern
         %stepnumber=2,               % Abstand zwischen den
         %Zeilennummern
         numbersep=5pt,              % Abstand der Nummern zum Text
         tabsize=2,                  % Groesse von Tabs
         extendedchars=true,         %
         breaklines=true,            % Zeilen werden Umgebrochen
         keywordstyle=\color{black},
         frame=b,         
 %        keywordstyle=[1]\textbf,    % Stil der Keywords
 %        keywordstyle=[2]\textbf,    %
 %        keywordstyle=[3]\textbf,    %
 %        keywordstyle=[4]\textbf,   \sqrt{\sqrt{}} %
         stringstyle=\color{white}\ttfamily, % Farbe der String
         showspaces=false,           % Leerzeichen anzeigen ?
         showtabs=false,             % Tabs anzeigen ?
         xleftmargin=17pt,
         framexleftmargin=17pt,
         framexrightmargin=5pt,
         framexbottommargin=4pt,
         %backgroundcolor=\color{lightgray},
         showstringspaces=false,      % Leerzeichen in Strings anzeigen
         language=prolog
 }

\newcommand{\interp}[1]{[\![ #1 ]\!]}
\newcommand{\hide}[1]{}

\title{Efficiency in Ambiguity:\\ Two Models of Probabilistic Semantics for Natural Language}
\author{}
\date{}

 \begin{document}
 % \author{Daoud Clarke and Bill Keller}
 % \institute{Daoud Clarke and Bill Keller \at Department of
 %   Informatics, University of Sussex, Falmer, Brighton,
 %   UK.\\ \email{\{d.clarke,~billk\}@sussex.ac.uk} }
\maketitle
\begin{abstract}
  Distributional accounts of meaning represent words as vectors or
  probability distributions over contexts and differ markedly from
  compositional, model-theoretic treatments of meaning such as
  Montague semantics. Recently, researchers have begun to address the
  problem of developing a unified account of natural language
  semantics that combines the strengths of both the distributional and
  compositional approaches. This paper argues for the adoption of
  probabilistic semantics as a basis for such a unifying account. We
  consider two different approaches. The first is a probabilistic
  generalisation of Montague semantics in which we assume a
  probability distribution over models. This approach has nice
  theoretical properties but does not account for the ubiquitous
  nature of ambiguity in natural language. This leads us to our second
  approach, which we call \emph{stochastic semantics}, in which we
  assume that a sequence of pairs of sentences and truth values are
  generated randomly. We demonstrate that learning and inference are
  feasible in both approaches, and that it is possible to learn that
  certain quantifiers reverse the direction of entailment.
\end{abstract}


\section{Introduction}

This paper argues for the adoption of probabilistic semantics as a
framework for combining distributional and logical semantics. The
success of lexical distributional representations of meaning has
recently lead to a concerted effort to describe compositionality
within distributional semantics
\cite{Widdows:08,Mitchell:08,Baroni2010,Garrette:11,Grefenstette:11,Socher:12,Lewis:13},
and efforts have been made to find a theoretical foundation for such
work \cite{Clarke:12,Kartsaklis:14}. In this paper, we argue that
existing work in probabilistic semantics
\cite{Gaifman:64,Nilsson:86,Sato:95} provides a rich seam of ideas
that can be applied to this problem.

The contributions of this paper are as follows:
\begin{enumerate}
\item We propose a framework for natural language semantics which
  combines Montague semantics with probabilistic semantics by
  specifying a probability distribution over Montague-style models.
\item We describe ongoing work to construct a probabilistic Montague
  semantics in which meanings are learnt from data, and demonstrate
  some desirable properties of this formalism. 
\item We show that restricting the space of models makes learning
  feasible, whilst retaining many of the desirable properties of full
  Montague semantics.
\item We show that taking account of lexical ambiguity leads to a new
  model in which pairs of sentences and truth values are generated
  randomly. This new approach gives a computationally more tractable
  formulation of the learning problem.
\item We show that both models are able to learn from a few examples
  that quantifiers such as ``no'' reverse the direction of entailment.
\end{enumerate}

Techniques which represent the meanings of words in terms of the contexts in
which they occur have become an indispensable tool in natural language
processing. Such distributional representations of meaning typically
represent words as vectors or probability distributions over
contexts. They have been applied to a wide variety of tasks, including word sense disambiguation~\cite{miller-EtAl:2012:PAPERS,khapra-EtAl:2010:ACL}, prepositional phrase attachment~\cite{Calvo05distributionalthesaurus}, textual entailment~\cite{berant-dagan-goldberger:2010:ACL}, co-reference resolution~\cite{lee-EtAl:2012:EMNLP-CoNLL}, predicting semantic compositionality~\cite{bergsma-EtAl:2010:EMNLP} and taxonomy induction~\cite{fountain-lapata:2012:NAACL-HLT}.

Current distributional approaches to the representation of meaning
differ in many respects from compositional, model-theoretic treatments
such as Montague semantics or Discourse Representation Theory
\cite{Blackburn:05}. Whilst distributional approaches provide an
account of word meaning, they have little to say about how those meanings
combine. In contrast, compositional approaches provide an account of the way in
which meanings combine to build a semantics for larger phrases, but
have little to say about the meanings of words or how they are
acquired. Recently, a number of researchers have begun to address the
problem of developing a unified account of natural language semantics
that combines the strengths of both the distributional and
model-theoretic approaches
\cite{Clarke:07,Coecke:10,Garrette:11,Lewis:13}. 

In the following we consider two different ways of constructing a
probabilistic semantics. Both approaches have the following desirable
properties:
\begin{itemize}
\item They are strongly grounded in probability theory: no ad-hoc methods
  or heuristics are used to combine distributional semantics with
  logical semantics.
\item They admit an implementation in which probability distributions
  can be learnt from data: the account is holistic, in that the
  learning of word meaning and composition are not separated, with the
  potential benefit of more flexible and accurate models of
  compositional distributional semantics.
\item Because both approaches describe a generative process,
  composition does not necessarily lead to a loss of meaning (for
  example by reducing the meaning of a sentence to a single truth
  value).
\end{itemize}

In the first approach we define a probability distribution over
Montague-style models in terms of a random generation process over
interpretations. This provides the basis of an elegant extension of
Montague semantics to the probabilistic domain: in particular it
combines the strengths of a compositional, model-theoretic account of
semantics, complete with a well-defined notion of entailment, with a
distributional account of the meaning of expressions.  However, this
approach carries a restrictive assumption that in interpreting any
given text, whilst the meaning of words may vary between different
interpretations, for any given interpretation the meanings are
fixed. Clearly, this is unrealistic as it is easy to construct
examples where the interpretation of a given word varies across the
text. For example, consider a text with two
occurrence of  the word ``bank'', one in the sense of
financial institution and one in the sense of the land adjoining a river.  In this  case we would clearly like
the interpretation to vary. Our second approach, which we call
\emph{stochastic semantics}, removes this restriction and allows for
ambiguity. This is consistent with distributional accounts of lexical
semantics, where the meaning of a polysemous word is identified with
the combination of all of its distributional contexts. Stochastic
semantics assumes a random generative process for pairs of sentences
and truth values, instead of models, which means that we have to
sacrifice some nice theoretical properties satisfied by the first
approach. A positive consequence of this approach however, is that
inference becomes more tractable since we can benefit from dynamic
programming.

We describe implementations of both approaches, and show that both are
able to perform learning and inference on small examples. For example,
they can learn that while ``some cats $X$'' entails ``some animals
$X$'', entailment is reversed with other quantifiers: ``no animals
$X$'' entails ``no cats $X$''.


% I'm not sure about referring to "distribution semantics" as it may
% be confused with distributional semantics - perhaps just refer to it
% as Sato's framework? Or else note the distinction.

% Yes - I agree with that - removed reference to distribution semantics.

\section{Background}

\subsection{Montague Semantics}

In a series of papers in the early 1970s \cite{Montague1970a,Montague1970b,Montague1973} the linguist and philosopher Richard Montague spelled out a formal treatment of the semantics of natural language. Montague's conception of semantics was truth-conditional and model-theoretic. He considered that a fundamental aim of any adequate theory of semantics was ``to characterise the notions of a true sentence (under a given interpretation) and of entailment'' \cite{Montague1970b}. A central methodological component of Montague's approach was the {\em Principle of Compositionality\/}, which states that the meaning of an expression is a function of the meanings of its parts and the way they are combined syntactically.


We assume, as in Montague semantics, that natural language expressions
are parsed by a categorial grammar. Further, we assume that with every word there is
associated a function with a type. Let $\mathcal{T}$ be the set of types defined
recursively, such that:
\begin{description}
\item [Basic types:] $e,t\in \mathcal{T}$
\item[Complex types:]  if $\alpha, \beta\in \mathcal{T}$, then $\alpha/\beta\in \mathcal{T}$.
\end{description}
Nothing else is in $\mathcal{T}$. Note that the type $\alpha/\beta$ denotes the type of a function from type
$\alpha$ to type $\beta$.

We define the set $B$ of \emph{basic expressions} to be a set of
symbols denoting meanings of words. We assume that associated with
each $b\in B$ there is a type $\tau_b$. The set $\Lambda$ of
\emph{meaningful expressions}, and the extension of $\tau$ to $\Lambda$ are
defined recursively as follows. Let $\Lambda$ be the smallest set such that:
\begin{itemize}
\item $B\subseteq \Lambda$
\item for every pair $\gamma,\delta\in \Lambda$ such that $\tau_\gamma
  = \alpha/\beta$ and $\tau_\delta = \alpha$, then $\gamma(\delta)\in
  \Lambda$ and $\tau_{\gamma(\delta)} = \beta$.
\end{itemize}
Let $\Lambda_\tau$ denote the set of meaningful expressions of type
$\tau$. A \emph{sentence} is a meaningful expression of type $t$.

% An
% \emph{interpretation} is a function $\phi$ which assigns to every
% sentence a value of true or false, denoted $\top, \bot$ respectively.

The set $D_\alpha$ of \emph{possible denotations} of type $\alpha$ is
defined recursively:
\begin{eqnarray*}
D_e &=& E\\
D_t &=& \{\bot,\top\}\\
D_{\alpha/\beta} &=& {D_\beta}^{D_\alpha}
\end{eqnarray*}
where $E$ is a set of \emph{entities}. Thus the denotation of a
complex type is a function between the denotations for the types from
which it is composed. An \emph{interpretation} is a pair $\langle E,
F\rangle$ such that $E$ is a non-empty set and $F$ is a function with
domain $B$ such that $F(b) \in D_{\tau_b}$ for all $b\in B$.

A meaningful expression $\gamma$ has the value $\interp{\gamma}$ in the
interpretation $\langle E, F\rangle$ defined recursively as follows:
\begin{itemize}
\item $\interp{b} = F(b)$ for $b\in B$
\item $\interp{\gamma(\delta)} = \interp{\gamma}(\interp{\delta})$ for $\gamma
  \in \Lambda_{\alpha/\beta}$ and $\delta \in \Lambda_\beta$.
\end{itemize}
A sentence $s$ is \emph{true} in interpretation $\langle E, F\rangle$
if $\interp{s} = \top$, otherwise it is \emph{false}.

A \emph{theory} is a set $T$ of pairs $(s,\hat{s})$, where $s$ is a
sentence and $\hat{s}\in\{\top,\bot\}$ is a truth value. A
\emph{model} for a theory $T$ is an interpretation $\langle E,
F\rangle$ such that $\interp{s} = \hat{s}$ for every sentence $s\in
T$. In this case we say that the model \emph{satisfies} $T$, and write
$\langle E, F\rangle \models T$.

\subsection{Probabilistic Semantics}
 
The idea of attaching probability to the truth of sentences is an old
one and is related to the foundation of probability itself
\cite{Keynes:21,Los:55}. \newcite{Gaifman:64} discusses probability
measures for first order calculus; the work of \newcite{Sato:95}
concerns probability measures for logic programs. The idea that we adopt from these approaches is to associate
probability measures with the space of models for the associated
logic. Our approach is closely related in terms of motivation to recent work by \newcite{Cooper:14}, 
which proposes a semantics based on a probabilistic type theory. 

%To our knowledge, such measures have not been studied for
% higher order logics, which is what we consider here.

A key idea in probabilistic semantics is to extend standard
model-theoretic accounts of meaning by assuming a probability
distribution over models.  In model-theoretic semantics, one way to
view the meaning of a logical sentence $s$ is as the set of all
interpretations $\mathcal{M}_s$ for which $s$ is true. Interpretations
in $\mathcal{M}_s$ are also called {\em models for\/} $s$. Then $s$
{\em logically entails\/} $t$ if and only if any model for $s$ is a
model for $t$: i.e. $\mathcal{M}_s \subseteq
\mathcal{M}_t$. Probabilistic semantics extends this idea by assuming
that models occur randomly. Informally, we may assume a probability
distribution over the set of models. Given such a probability
distribution we can then estimate the probability of a sentence $s$ as
the sum of the probabilities of all models $\mathcal{M}_s$, for $s$.

Formally, for the sake of generality, probabilities are defined in
terms of a probability space $\langle \Omega, \sigma, \mu\rangle$,
where $\Omega$ is the set of all models, $\sigma$ is a sigma algebra
associated with theories and $\mu$ is a probability measure on
$\sigma$.

%\subsection{Example: Fine-grained Meanings}


% One distinguishing feature of distributional semantics is its ability
% to describe fine-grained aspects of meaning: the continuous nature of
% the space allows arbitrary degrees of similarity. We use the example
% from \cite{Clark:08}, computing the similarity of the two sentences
% \emph{John likes Mary} and \emph{John loves Mary}.

% We assume the following types for basic expressions:
% \begin{eqnarray*}
% \mathit{john},\mathit{mary} & \in & \Lambda_{(e/t)/t}\\
% \mathit{likes}, \mathit{loves} & \in & \Lambda_{(((e/t)/t)/((e/t)/t))/t}
% \end{eqnarray*}
% Proper nouns are interpreted as sets of sets of entities (functions
% from functions from entities to truth values to truth values), while
% verbs are functions from pairs of sets of sets of entities to truth
% values.

\begin{table*}
  \parbox{.45\linewidth}{
    \centering
    \begin{tabular}{|l|l|l||c|c|c|c|}
      \hline
      \emph{subject} & \emph{verb} & \emph{object} & $m_1$ & $m_2$ & $m_3$ & $m_4$\\
      \hline
      john & likes & john & 1 & 1 & 1 & 1\\
      john & likes & mary & 0 & 1 & 1 & 1\\
      mary & likes & john & 1 & 0 & 0 & 1\\
      mary & likes & mary & 0 & 0 & 1 & 1\\
      john & loves & john & 1 & 1 & 1 & 1\\
      john & loves & mary & 1 & 0 & 1 & 1\\
      mary & loves & john & 1 & 1 & 1 & 1\\
      mary & loves & mary & 1 & 0 & 1 & 1\\
      \hline
    \end{tabular}
    % \begin{tabular}{ccc}
    %   \hline
    %   a&b&c\\
    %   \hline
    % \end{tabular}
    \caption{Four possible models describing relationships between John
      and Mary.}
    \label{table:models}
  }
  \hfill
\parbox{.45\linewidth}{
  \centering
  \def\arraystretch{1.5}%
  \begin{tabular}{c}      
    John likes Mary \\ $\mu(\{m_2, m_3, m_4\}) =
    P(m_2) + P(m_3) + P(m_4) = 0.9$\\
    \hline
    Mary likes John or Mary \\ $\mu(\{m_1, m_3, m_4\}) =
    P(m_1) + P(m_3) + P(m_4) = 0.8$\\
    \hline
    John likes Mary given that he loves her\\
    $\mu(\{m_3, m_4\})/\mu(\{m_1, m_3, m_4\}) = 0.7/0.8$\\
  \end{tabular}
  % \begin{tabular}{ccc}
  %   \hline
  %   d&e&f\\
  %   \hline
  % \end{tabular}
  \caption{Statements and their probabilities given the models in
    Table \ref{table:models}.}
  \label{table:statements}
}
\end{table*}

% \begin{table}
%   \begin{minipage}[b]{.5\linewidth}
%     % \centering
%     \label{table:models}
%   \end{minipage}%
%   \begin{minipage}[b]{.5\linewidth}
%     % \centering
%     \subcaption{Another subfigure}\label{fig:1b}
%   \end{minipage}
%   \caption{A figure}\label{fig:1}
% \end{table}

% For simplicity of exposition, and to make the number of
% interpretations we have to consider smaller, we will give some
% properties which will be fixed for all interpretations under
% consideration, i.e.~interpretations which do not satisfy the given
% property will be given probability zero. For example, we assume that
% the set $E$ of entities is fixed, $E = \{\mathbf{john},
% \mathbf{mary}\}$, and that the interpretation of nouns is fixed:
% $$\mathit{john}(x) = \begin{cases}
% \top & \text{if } x(\mathbf{john}) = \top\\
% \bot & \text{otherwise}
% \end{cases}$$ i.e.~$\mathit{john}$ is the set of all sets containing
% the entity $\mathbf{john}$, and similarly for $\mathit{mary}$, if we
% interpret functions with range $\{\top,\bot\}$ as indicator
% functions. We further assume that $\mathit{likes}$ and
% $\mathit{loves}$ behave logically so that if ``John likes $x$'' is
% true and ``Mary likes $x$'' is true then ``John and Mary like $x$'' is
% also true (note that this isn't true for all verbs; consider ``John
% and Mary played chess''). Similarly we assume that if ``$x$ likes
% John'' and ``$x$ likes Mary'' are true then ``$x$ likes John and
% Mary'' is true.

% Four possible models $m_1, m_2, m_3, m_4$ under these assumptions are
% described in Table \ref{table:models}. At this point our example
% diverges from standard model-theoretic semantics. 

In general, the set of models will be infinite, but for the purposes
of exposition, it helps to consider a simple example with a fixed
number of models, such as that described by Table
\ref{table:models}. Each of the columns $m_1$ to $m_4$ describe a
different model describing relationships between John and Mary.  We
assume a probability distribution over the set of models, with $P(m_1)
= 0.1$, $P(m_2) = 0.2$, $P(m_3) = 0.3$, $P(m_4) = 0.4$ and all other
possible models have zero probability. Using this set of models we can
deduce the probability of statements regarding John and Mary, some
examples are given in Table \ref{table:statements}.

% We assume that the categorial grammar assigns the meaningful
% expression $\mathit{likes}(\mathit{mary})(\mathit{john})$ to the
% sentence ``John likes Mary''. The probability of this sentence is
% \begin{eqnarray*}
% \mu(M(\{\mathit{likes}(\mathit{mary})(\mathit{john})\})) &=&
% \mu(\{m_2, m_3, m_4\})\\
% &=&0.9
% \end{eqnarray*}
% Similarly, the models which are true for the sentence ``John loves
% Mary'' are $\{m_1, m_3, m_4\}$, having probability 0.8. The models
% which satisfy both of these sentences are $\{m_3, m_4\}$ with
% probability 0.7, so the
% degree to which ``John loves Mary'' entails ``John likes Mary'' is
% $0.7/0.8$. Note that in this case there is no logical entailment, but
% there is a high degree of entailment.

% \subsection{Example: Conjunction and Disjunction}

% Our formalism inherits the nice properties of model-theoretic
% semantics with respect to conjunction and disjunction. We describe the
% conjunction of nouns with the basic expression $\mathit{and}$ of type
% $(((e/t)/t)/((e/t)/t))/((e/t)/t)$: it is a function taking two sets of
% sets of entities, and returning another set of sets of
% entities. Specifically we assume that it returns the intersection of
% the two sets. Similarly $\mathit{or}$ is a function of the same type,
% returning the union of the two sets.

% Then $\mathit{and}(john)(mary)$ is the set of sets containing both
% $\mathbf{john}$ and $\mathbf{mary}$. Assume the categorial grammar
% gives the meaningful expression
% $\mathit{likes}(\mathit{and}(\mathit{john})(\mathit{mary}))(\mathit{mary})$
% to the sentence ``Mary likes John and Mary''. The set of models
% satisfying this sentence is $\{m_4\}$, so this sentence has
% probability 0.4. The set of models satisfying
% $\mathit{likes}(\mathit{or}(\mathit{john})(\mathit{mary}))(\mathit{mary})$
% for ``Mary likes John or Mary'' is $\{m_1, m_3, m_4\}$, so this
% sentence has probability 0.8. Note that the logical entailments we
% would expect from logical semantics are preserved: the degree to which
% ``Mary likes John and Mary'' entails ``Mary likes John'' is 1.

%  In this example, we
% assume that meanings are described by a measure space for each part of
% speech. Measure spaces for larger constituents are then constructed as
% product measures \cite{Taylor:06} of their constituent measure
% spaces. This idea is an extension of the observation of
% \citet{Keenan:85} that there is a Boolean algebra associated with each
% constituent.

% A measure space can be constructed for a particular part of speech
% directly from the distributional semantic representations of the
% associated words, or from an ontology with frequency information
% \cite{Clarke:07}.


\section{A Probabilistic Montague Semantics}

\hide{In this section we describe our first model of a probabilistic
semantics for natural language. This approach  generalises Montague semantics to
the probabilistic domain in a natural way. 
}
Let $\Omega$ be the set of all Montague-style interpretations. Let
$M(T)$ denote the set of all models for the theory $T$ and let
$\sigma_0$ be the set of all sets of models that satisfy some theory:
$$\sigma_0 = \{M(T) : T\text{ is a theory}\}$$
In general, $\sigma_0$ is not a sigma algebra; in particular we cannot
guarantee that for two theories $T_1$
and $T_2$, we have
$M(T_1)\cup M(T_2) \in \sigma_0$ . For any Montague grammar which contains a propositional
fragment, this would hold, however even if this is not the case, we
can still define a probability space by considering the sigma algebra
$\sigma$ generated by $\sigma_0$; this is defined as the smallest
sigma algebra containing $\sigma_0$. Let $\mu$ be a probability measure on $\sigma$; then
$\langle\Omega,\sigma,\mu\rangle$ is a probability space which
describes the probability of theories. The probability of $T$ is
$\mu(M(T))$.

% A \emph{theory} is a set $T$ of pairs $(s,\hat{s})$, where $s$ is a
% sentence and $\hat{s}\in\{\top,\bot\}$ is a truth value. A
% \emph{model} for a theory $T$ is an interpretation which assigns
% $\hat{s}$ to every sentence $s\in T$. In this case we say that the model \emph{satisfies} $T$. Let $\Omega$ be the set of all
% interpretations, and $\sigma$ be the sigma algebra consisting of the
% set of all sets of models that satisfy some theory. Let $\mu$ be a probability measure
% on $\sigma$; then $\langle\Omega,\sigma,\mu\rangle$ is a probability
% space which describes the probability of theories. Let $M(T)$ denote
% the set of all models for the theory $T$; the probability of $T$ is
% $\mu(M(T))$.

Given two sentences $s_1$ and $s_2$, we can compute the conditional
probability of $s_1$ given $s_2$ as
$$\frac{\mu(M(\{(s_1, \top), (s_2, \top)\}))}{\mu(M(\{(s_2,\top)\}))}$$
We interpret this conditional probability as the {\em degree to which $s_2$ entails $s_1$\/}. Note that if $s_2$ logically entails $s_1$ then the degree to which  $s_2$ entails $s_1$ is $1$.

% In this paper, we consider the situation where the logical language
% $L$ is restricted to that of logic programs, i.e.~Horn clauses with
% universal quantification, and we use Herbrand models. In this case,
% the distribution can be defined in terms of minimal models, as
% demonstrated by Sato (\citeyear{Sato:95}), meaning that probabilities
% of logical sentences can be computed efficiently. He also shows that
% in this case distributions can be efficiently learnt from data using
% the Expectation Maximisation (EM) algorithm. We demonstrate how this
% can work for toy data consisting of natural language sentences.

%\section{Learning}

%\section{Examples}

% One can envisage many possible ways of describing probability measures
% $\mu$. Our main goal is to be able to learn such measures from corpus
% data, and we describe one possible approach to doing this in Section
% \ref{section:distributions}. However, in this section, to demonstrate
% the potential for our approach, we give examples in which measures are
% given explicitly.
 
\subsection{Restricting the Space of Models}
\label{section:distributions}

A key objective is to be able to learn semantics from corpus data,
generalising lexical distributional semantics. In this section, we
propose one way that this may be achieved within our framework. The
central idea is to limit the number of denotations under
consideration and define a probabilistic generative model for
interpretations. Specifically we assume that $E$ is fixed. Let
$\phi_\tau = \{\interp{\lambda} : \lambda\in \Lambda_\tau\}$
be the set of denotations that occur with type $\tau$. We assume that
$F$ is constrained such that $|\phi_\tau| = n_\tau$ where $n_\tau$
is a constant for each type satisfying $n_\tau \le |D_\tau|$. This
restriction in the number of occurring denotations makes learning a
distribution over models feasible. We also assume that the occurring
denotations are ordered so that we can write $\phi_\tau =
\{d_{\tau,1}, d_{\tau,2}, \ldots d_{\tau, n_\tau}\}$.


We assume that denotations are generated with probabilities
conditionally independent given a random variable taking values from
some set $H$. This gives us the following process to generate $F$:
\begin{itemize}
\item Generate a hidden value $h\in H$
\item Generate a value $F(b) \in \phi_{\tau_b}$ for each $b\in B$ according to
  $P(d_{\tau_b,i}|b, h) = \theta_{b,i,h}$
\item For each pair of types $\alpha/\beta, \beta$:
\begin{itemize}
\item Generate a value $\interp{d_{\alpha/\beta,i}(d_{\alpha,j})}$
  according to $P(d_{\beta,k}|d_{\alpha/\beta,i}, d_{\alpha,j},h) = \theta_{\beta,i,j,k,h}$
\end{itemize}
\end{itemize}
The parameters to be learnt are the probability distributions
$\theta_{b,i,h}$ for each basic expression $b$ and hidden value $h$
over possible values $d_{\tau_b,i}$, and $\theta_{\beta,i,j,k,h}$ over
values $d_{\beta,k}$ for each function $d_{\alpha/\beta,i}$, argument
$d_{\alpha,j}$, and hidden value $h$.

\subsection{Learning and Inference}

Given a theory $T$ and parameters $\theta$, we can compute the
probability $P(T|\theta)$ using the following algorithm:
\begin{itemize}
\item For each hidden variable $h$:
\begin{itemize}
\item Iterate over models for $T$. This can be done in a bottom-up
  fashion by first choosing the denotation for each basic expression,
  then recursively for complex expressions. Choices that are made have
  to be remembered, and any time a contradiction is detected, the
  model is abandoned.
\item The probability of each model given $h$ can be found by
  multiplying the parameters associated with each choice made in the
  previous step.
\end{itemize}
\end{itemize}

We can use Maximum Likelihood to estimate these parameters given a set
of observed theories $\mathcal{D} = \{T_1, T_2, \ldots T_N\}$. We look
for the parameters $\theta$ that maximize the likelihood
$$P(\mathcal{D}|\theta) = \prod_{n=1}^N P(T_i|\theta)$$
This can be maximised using gradient ascent or expectation
maximisation. Our current implementation uses the L-BFGS-B algorithm
in SciPy \cite{Zhu:97}.

\section{Stochastic Semantics}

The formulation of probabilistic Montague semantics presented above does not
adequately account for ambiguity in natural language. Although we have
a probability distribution over interpretations, in any given
interpretation, each occurrence of a word must have the same
denotation. This is intuitively unsatisfactory, since we can easily
conceive two sentences where the same word has different meanings. The
standard model-theoretic approach to this problem would be to
disambiguate words before performing inference. This approach is
however inconsistent with
distributional  semantics, in which \emph{degrees} of
ambiguity are incorporated into the representation of a word's
meaning.

Our solution is to drop the requirement that in any given interpretation, each occurrence of a word must
have the same denotation. If we generalise this idea to denotations
for all types, then we end up with an entirely different semantic
model. It turns our that this model can easily be formulated within the framework due to
\newcite{Sato:97}. We call our new framework \emph{stochastic semantics}. We no longer
associate a set of models with a sentence. Instead of having a
generative model for models, we now assume that pairs of sentences and
truth values are randomly generated in sequence. The probability of
each pair being generated is conditionally independent of previous
pairs given the hidden variable.

% in which we have a probability distribution over
% \emph{theories} instead of models. We can define a generative
% model for theories, analogous to the one for models, as follows:
% \begin{itemize}
% \item Choose a hidden value $h$
% \item Choose a number $n$ of sentences according to some distribution
%   (e.g. Poisson)
% \item Do $n$ times:
% \begin{itemize}
% \item For each basic expression occurrence $b$ in $s$, generate a
%   value in $\phi_{\tau_b}$ according to $P(d_{\tau_b,i}|b, h) =
%   \theta_{b,i,h}$
% \item For each expression of the form $\gamma(\delta)$, where $\gamma$
%   and $\delta$ have values $d_{\tau_\gamma,i}$ and $d_{\tau_\delta,j}$
%   respectively, choose a value in $\phi_{\tau_{\gamma(\delta)}}$
%     according to $P(d_{\tau_{\gamma(\delta)},k}|d_{\tau_\gamma,i},
%       d_{\tau_{\delta},j}) = \theta_{\tau_{\gamma(\delta)},i,j,k,h}$
% \end{itemize}
% \end{itemize}

\begin{figure}
\centering
\begin{lstlisting}
% Hidden variable
values(hidden, [h0, h1]).

% Types
values(word(noun, Word, Hidden), [n0, n1]).
values(word(verb, Word, Hidden), [v0, v1]).
values(word(det, Word, Hidden), [d0, d1]).
values(function(s, Value1, Value2, Hidden), [t0, t1]).
values(function(np, Value1, Value2, Hidden), [np0, np1]).
values(function(vp, Value1, Value2, Hidden), [vp0, vp1]).

evaluate(w(Word, Type), Hidden, Result) :-
	msw(word(Type, Word, Hidden), Result).
evaluate(f(Type, X, Y), Hidden, Result) :-
	evaluate(X, Hidden, XResult),
	evaluate(Y, Hidden, YResult),
	msw(function(Type, XResult, YResult, Hidden), Result).

theory([], _).
theory([truth(Sentence, Result)|Tail], Hidden) :-
	evaluate(Sentence, Hidden, Result),
	theory(Tail, Hidden).

theory(T) :-
	msw(hidden, Hidden),
	theory(T, Hidden).
\end{lstlisting}
\caption{A PRISM program describing probability distributions over
  natural language models used for our examples.}
\label{figure:program}
\end{figure}

\begin{figure}
\begin{lstlisting}
:- prob(theory([
  truth(f(s, f(np, w(the, det),
                   w(cat, noun)),
             f(vp, w(likes, verb),
                   f(np, w(the, det),
                         w(dog, noun)))),
        t1)]), X).
\end{lstlisting}
\caption{PRISM query to evaluate the probability that the sentence
  \emph{the cat likes the dog} is true.}
\label{figure:query}
\end{figure}



\subsection{Implementation}


An implementation for our approach is shown in Figure 
\ref{figure:program}. The code listed in the figure is written in PRISM \cite{Sato:97}, a probabilistic extension of the Prolog logic-progamming language. Note that to simplify the code, complex types of the underlying Montague grammar are referred to by their corresponding natural language
categories. Thus, type $t$ is represented by \texttt{s} (a sentence), type $(e/t)$ by \texttt{vp} (a verb phrase) and type $(e/t)/t$ by \texttt{np} (a noun phrase). 
\hide{
The PRISM language augments Prolog with probabilistic predicates and declarations. In particular, it incorporates random switches. The predicate \texttt{values} is used to declare a named switch and associate with it a number of possible outcomes while the probabilistic predicate \texttt{msw} allows a random choice to made amongst these outcomes during execution. 
}
The PRISM program shown in Figure \ref{figure:program} describes how sentences are randomly assigned truth values using probabilistic switch statements. For example, the declaration 
\begin{center}
\texttt{values(word(noun,Word,Hidden),[n0,n1])} 
\end{center}
introduces a switch for nouns having two possible outcomes, \texttt{n0} and \texttt{n1}.
The outcomes are conditioned on the particular choice of noun (\texttt{Word}) and on the choice of hidden variable (\texttt{Hidden}). Similarly, the \texttt{values} predicate is used to declare switches associated with complex types. For example the values associated with a sentence (\texttt{s}) switch are conditioned on the choices of its component parts and a hidden variable.

The probability of a theory is derived by evaluating the truth
conditions of the sentences that make it up.\hide{Note that probability
distributions are defined over models, not theories, so that to
evaluate the probability of a theory it is necessary to sum over the
probabilities of all of its satisfying models. To achieve this, the
predicate \texttt{prob} is used.} An example query returning the
probability of the sentence {\em the cat likes the dog\/} is shown in
figure \ref{figure:query}. This is expressed as a theory
$\{(\mathit{likes}(\mathit{the}(\mathit{dog}))(\mathit{the}(\mathit{cat})),\top)\}$,
which can be translated into a Prolog expression in a straightforward
manner.


\section{Complexity}

\subsection{Complexity of Probabilistic Montague Semantics}

\begin{definition}[PM-SAT]
  We define the problem of probabilistic Montague semantics
  satisfiability (PM-SAT) as follows: given a restricted probabilistic
  Montague semantics for a language, that is, a restricted set of
  denotations $\phi_\tau$ for each type $\tau$ and probability
  distributions defined by $\theta$, determine whether the probability
  of a given sentence taking the value $\top$ is nonzero.
\end{definition}

\begin{theorem}
  \emph{PM-SAT} is NP-complete with respect to the length of the input
  sentence.
\end{theorem}

\begin{proof}
We can show NP-hardness by reduction from SAT. We construct a special
language for propositional logic with symbols $x_i$ of type $t$,
symbols $\land$ and $\lor$ of type $(t/t)/t$ and $\lnot$ of type
$t/t$. We do not need the hidden values, so we can assume $H$ has only
one element. We allow the variables $x_i$ to take values in $\{\top, \bot\}$
with equal probability. We allow two denotations $d_{(t/t)/t, \land}$
and $d_{(t/t)/t, \lor}$ for type $(t/t)/t$, and $\land$ and $\lor$
take these denotations respectively with probability 1. We allow five
denotations for type $t/t$, one for $\lnot$, and two each for the
intermediate values taken by $\land$ and $\lor$ after operating on
either $\top$ or $\bot$. It is then straightforward to define
probabilities to match the rules of propositional logic: probability
values are just 0 or 1 depending on the appropriate truth table for
the operator under question. Then a sentence has truth value $\top$
with nonzero probability if and only if it is satisfiable.

PM-SAT can be solved in linear time by a non-deterministic machine by
choosing a value from $H$, assigning every possible value to all basic
expressions and then recursively to complex expressions. Any
assignment that gives a non-zero probability for the sentence for the
sentence taking the value $\top$ will return true. This implies that
PM-SAT is in NP, and is thus NP-complete.
\end{proof}

Let us call the problem of computing the probability of a sentence in
probabilistic Montague semantics PM-PROB. Clearly PM-PROB is at least
as hard as PM-SAT, since if we knew the probability of a sentence we
would know whether it was nonzero; thus PM-PROB is NP-hard.

\subsection{Complexity of Stochastic Semantics}

Because there is no dependency between different parts of the sentence
in stochastic semantics, given a fixed hidden value, the probability
of a sentence can be computed much more efficiently than in
Probabilisticy Montague Semantics. To do this, we use dynamic
programming, storing the probability distribution over the possible
denotations of each expression so that they are only computed once for
each hidden value. Let $L$ be the number of expressions in a sentence,
$n$ the maximum number of denotations for all types, i.e.~the greatest
value of $n_\tau$ for all types $\tau$. The algorithm to compute the
probability of a sentence is as follows:
\begin{itemize}
\item For each $h\in H$:
\begin{itemize}
\item For each basic expression of type $\tau$, compute the
  probability distribution over $\phi_\tau$; this can be computed in
  maximum $O(n)$ time.
\item Recursively for each complex expression of type $\tau$, compute
  the probability distribution over $\phi_\tau$, this requires maximum
  $O(n^2)$ time since we need to iterate over possible values of the
  expression and the type it acts on.
\end{itemize}
\end{itemize}
Computing the probability of the sentence thus requires $L$
computations each of complexity at most $n^2$; this must repeated
$|H|$ times for each hidden value in $H$. Thus the total worst-case
time complexity is $O(|H|Ln^2)$.



\section{Learning Reversal of Entailment}

We verified that our two systems were able to learn some simple
logical features of natural language semantics, in particular that
determiners such as ``no'' reverse the direction of entailment, so
that while ``some cats'' entails ``some animals'', ``no animals''
entails ``no cats'' (Table \ref{table:mono}).

% We have demonstrated the feasibility of our approach by showing that
% it can learn the monotonicity of determiners on small examples (Table
% \ref{table:mono}). Our system is able to correctly predict that
% \emph{no people} entails \emph{no men}, having seen other example
% entailments.

\begin{table*}
  \parbox{.62\linewidth}{
    \begin{center}
      \begin{tabular}{|l|l|l|}
        \hline
        Text & Hypothesis & Ent.\\
        \hline
        some cats like all dogs & some animals like all dogs & Yes\\
        no animals like all dogs & no cats like all dogs & Yes\\
        some dogs like all dogs & some animals like all dogs & Yes\\
        no animals like all dogs & no dogs like all dogs & Yes\\
        some men like all dogs & some people like all dogs & Yes\\
        \hline
        no people like all dogs & no men like all dogs & Yes\\
        no men like all dogs & no people like all dogs & No\\
        \hline
      \end{tabular}
      \caption{Text and Hypothesis sentences, and whether entailment
        holds. We expect that a system should be able to learn from
        the data above the line that the determiner ``no'' reverses the
        direction of entailment.}
      \label{table:mono}
    \end{center}
  }
  \hfill
  \parbox{.32\linewidth}{
    \centering
    \csvautotabular{lexical.csv}
    \caption{Learnt probabilities obtained using the
      stochastic semantics implementation.}
    \label{table:lexical-sato}
  }
\end{table*}


For each example entailment pair for which entailment holds (the
``text'' entails the ``hypothesis''), we add
the following theories to $\mathcal{D}$:
\[
\{(s_T,\top),(s_H,\top)\}, \{(s_T,\bot),(s_H,\top)\},\{(s_T,\bot),(s_H,\bot)\}
\]
Where $s_T$ is the sentence associated with the text and $s_H$ the
sentence associated with the hypothesis. We then learn the optimal
parameters $\theta$ given this data, and use these to compute the
degree of entailment on unseen pairs of sentences.

Our system correctly predicted the reversal of entailment. Pairs where entailment
holds were given a degree of entailment of 1.0 by our
system. For other pairs, the degree of entailment assigned was  0.5. We did not see any difference between the two
systems on these small examples.

%\subsection{Examples}


% The PRISM framework is equipped with a proof-theoretic approach to learning so that it is possible to estimate parameters from observed data. A number of different learning algorithms are implemented within the framework. To estimate the
% parameters in the following examples we used the standard expectation maximisation algorithm.

% Table \ref{table:lexical} demonstrates that our system can learn
% simple lexical entailment rules. In this case it is learnt that \emph{cat} entails
% \emph{animal} on the basis of a small number of examples. Table \ref{table:quantifiers} demonstrates that our system can learn
% something more significant: it can learn that the entailment direction
% is reversed with certain quantifiers. Quantifiers such as \emph{some} are
% \textbf{monotonically increasing}, so that \emph{some cats} entails
% \emph{some animals}. Quantifiers like \emph{no} and \emph{all} are
% \textbf{monotonically decreasing}, so that \emph{no animals} entails
% \emph{no cats}. Our system is able to determine this from very little
% training data.

% \begin{table*}
% \centering
% \begin{tabular}{|l|l|l|}
% \hline
% Text & Hypothesis & Ent\\
% \hline
% \emph{the cat chased the dog} & \emph{the animal chased the dog} & \\
% \emph{the cat likes the dog} & \emph{the animal likes the dog} & \\
% \hline
% \emph{the cat loves the dog} & \emph{the animal loves the dog} & 1.0\\
% \emph{the animal loves the dog} & \emph{the cat loves the dog} & 0.5\\
% \hline
% \end{tabular}
% \caption{Learning lexical entailment from examples, showing training
%   data at the top, and test data below, with the degree of entailment
%   determined by our system.}
% \label{table:lexical}
% \end{table*}

% \begin{table*}
% \centering
% \begin{tabular}{|l|l|l|}
% \hline
% Text & Hypothesis & Ent\\
% \hline
% \emph{some cats like all dogs} & \emph{some animals like all dogs} & \\
% \emph{no animals like all dogs} & \emph{no cats like all dogs} & \\
% \emph{some men like all dogs} & \emph{some people like all dogs} & \\
% \hline
% \emph{no people like all dogs} & \emph{no men like all dogs} & 1.0\\
% \emph{no men like all dogs} & \emph{no people like all dogs} & 0.5\\
% \emph{most people like all dogs} & \emph{most men like all dogs} & 0.75\\
% \hline
% \end{tabular}
% \caption{Learning how quantifiers reverse the entailment direction,
%   with training data at the top, and test data below, with the degree
%   of entailment determined by our system.}
% \label{table:quantifiers}
% \end{table*}

\subsection{Recovering Lexical Semantics}
\label{sec:recovering}

Our formalism does not make use of any explicit
representation of lexical semantics. However, distributional
representations of word meanings can be recovered from the learnt
models. The representation of a word in this case is a matrix, rather
than a vector, because there is a different probability distribution
over the possible values for a particular type corresponding to each
hidden value. For example, Table \ref{table:lexical-sato} shows the
learnt values for the nouns obtained using the stochastic semantics
implementation. Allowing the distribution to vary with the hidden
variable is precisely what enables learning of the behaviour of
quantifiers. Without this flexibility learning would not be possible.

If we want to compare words in a manner similar to vector based
representations of meaning, we can consider the matrix entries as a
flat vector. In this case we can use any of the standard similarity measures (e.g. 
cosine) to compare word representations.

\section{Relation to Probabilistic Lexical Semantics}

% Bismillahi-r-Rahmani-r-Rahim

Our model can be related to probabilistic models of lexical semantics
such as latent Dirichlet allocation (LDA) \cite{Blei:03} and
probabilistic latent semantic analysis \cite{Hofmann:99}. These are
generative models that describe the probability of a document as the
product of the probability of the terms making up the document, where
the probability of the terms is conditioned on one or more hidden
variables. These models do not take account of the order of the words
in the document.

\begin{figure}
\includegraphics[width=\linewidth]{LDA.pdf}
\caption{Latent Dirichlet allocation adapted to generate truth values.}
\label{figure:lda}
\end{figure}

Figure \ref{figure:lda} shows how LDA can be adapted to generate truth
values and words simultaneously. The probability of a word, truth
value pair $(w,t)$ being generated is conditioned on the topic $z$
generated for that word; in standard LDA only a word is generated. The
rest of the model remains the same.

If we consider words as sentences and documents as theories, then,
this ``truth value LDA'' can be viewed as an instance of our
stochastic semantics.

\section{Discussion}

The two models of semantics we have discussed are closely
related.
% It is notable that Sato's framework is also based around a probability
% distribution over models. However, because of its foundation in logic
% programming, only \emph{Herbrand} models are considered.
The semantics
of the switches in the PRISM language means that they are assigned
a random value on each evaluation. This is why a term may have different meanings
within a single interpretation. In contrast, the model based on
Montague semantics would require switches to take the same value on
each evaluation, which is not directly supported by PRISM.

The flexibility granted by stochastic semantics results
in the loss of certain nice properties. Thus, for
probabilistic Montague semantics, it is easy to verify that any given sentence must entail itself to degree 1. This is
no longer guaranteed in our model of stochastic semantics. Indeed
the failure of logical entailment in this case is a necessary consequence of incorporating stochastic
ambiguity into the formalism. In standard model-theoretic semantics,
it may be assumed that sentences are disambiguated before any reasoning is
performed. If we do not disambiguate however, then a sentence may not entail itself to degree 1 since it may mean different things
in different contexts. We argue that it makes sense to
sacrifice some properties which are expected when handling logical
expressions in order to account for ambiguity as
as an inherent property of language.

We note too that the two models also have different computational properties.
In particular, stochastic semantics allows for efficient computation with dynamic
programming. This is possible because of the re-evaluation of switch
values: each evaluation is independent of previous ones. In contrast,
in probabilistic Montague semantics, we need to remember choices made
previously, which means we lose the independence property in general,
making dynamic programming much less useful. 



% \subsection{Application to Textual Entailment}

% The task of recognising textual entailment \cite{Dagan:05} is to
% determine, given two sentences $T$ (the \emph{text}) and $H$ (the
% \emph{hypothesis}), whether $T$ {\em textually entails\/} $H$. It is important to note that the notion of
% textual entailment is an informal one based on human judgments. This makes textual entailment much closer to the notion of degree of entailment available within our probabilistic approach than that of strict logical entailment.  The importance of textual entailment within natural language processing lies in its broad relevance to a range of tasks. It represents a core problem, a solution to which is required in order to develop applications in areas such as
% information retrieval, machine translation, question answering and
% summarisation.

% Previous attempts to apply logical representations of meaning to this
% task have applied {\em ad hoc\/} heuristics to handle the lack of robustness in
% standard approaches. \citet{Bos:06} used model builders to construct a
% model for $T$ and a model for $T$ and $H$ together and used the size
% of these models as an indication of the degree of entailment. The
% intuition here is that if the model for $T$ and $H$ together is larger
% than that for just $T$, then $H$ contains a lot of knowledge not in
% $T$ and there is a low degree of entailment.  This idea can be
% directly related to our approach: it amounts to assuming the existence of a probability
% distribution over models,
% with larger models less probable than smaller ones.

% Textual entailment datasets can also be used to learn the parameters
% described in the previous section. A dataset typically consists of a
% set of natural language sentence pairs, each annotated as to whether
% entailment holds or not. Suppose we are given a pair $(T,H)$ with associated meaningful expressions $s_T$ and $s_H$. In the case that entailment holds for $T$ and $H$ then the dataset will contain the following theories as observations:
% \[
% \{(s_T,\top),(s_H,\top)\}, \{(s_T,\bot),(s_H,\top)\},\{(s_T,\bot),(s_H,\bot)\}
% \]
% In the case that entailment does not hold, on the other hand, then the dataset will contain just the following as an observation:
% \[
% \{(s_T,\top),(s_H,\bot)\}
% \]

% This
% idea could be used as a supervised approach to the general task of recognising textual
% entailment.  However, we leave the evaluation of this idea to future
% work.

\section{Related Work}

%\cite{Clarke:07,Coecke:10,Garrette:11,Lewis:13}. 

In recent work, Cooper et al.~\cite{Cooper:14} (see also
\cite{Eijck:12}) have described a probabilistic semantics for natural language in which probability is distributed over situation types~\cite{Barwise:83}. They propose a rich type system in which the judgment about whether a given situation is of a given type is probabilistic. Rather than assigning truth-conditions to sentences, they present a compositional interpretation function that assigns probability-conditions. The work of Cooper et al shares similar motivations to ours. In particular, it aims to account for the gradience of semantic judgements (for example, gradations of entailment) and stresses the importance of providing an account of semantic learning within language acquisition.

Their approach differs from ours in a number of key respects. First, they define probability over situation types and provide a probabilistic interpretation function. In contrast, we define a generative process over interpreting structures so that probabilities of meaning are defined in terms of probability distribution over models. Second, they describe an approach to semantic learning that makes use of observations of situations. Our approach is to learn from pairs of sentences and truth values directly. The result of their learning is an assignment of probabilities to situations; the result of our learning is the ability to determine degrees of entailment between pairs of sentences. Third, our approach can be related to distributional semantics: as output of our learning procedure we can extract lexical representations as matrices.

In other related work, Coecke et al.~\cite{Coecke:10} have proposed a framework based on category-theoretic
similarities between vector spaces and pregroup grammars. Their
approach is closely related to ours since it is also
founded in Montague semantics: words are treated as linear functions
between vector spaces. Each word is represented as an element of a
tensor product space, and a pregroup grammar is used to determine how
these are composed using the inner product. It was recently
demonstrated that this approach can be extended to allow the
simulation of predicate calculus using
tensors \cite{Grefenstette:13}.

There are two main differences between their approach and ours. First,
in the language of Montague semantics, they assume a single
model. This means that composition necessarily loses information about
the composed words, and this is evident in the use of the inner
product for composition. Sentences are all described in a space of
fixed dimensionality, no matter how long or complex they are. In our
approach, a sentence describes a probability distribution over models,
which is an infinite set, allowing for the representation of arbitrary
complexity.

The second difference is that their approach is vector-based whereas
ours is probabilistic in nature. Thus their approach is more
closely related to approaches to lexical distributional semantics
which are vector based, such as latent semantic analysis
\cite{Deerwester:90} and random indexing \cite{Sahlgren:02}, whereas
ours is closer to probabilistic approaches such as latent Dirichlet
allocation \cite{Blei:03}.

Garrette et al.~\cite{Garrette:11} describe an approach to combining logical
semantics with distributional semantics using Markov Logic Networks
\cite{Richardson:06}. Sentences are parsed into logical form using
Boxer \cite{Bos:04}, and probabilistic rules are added using the
distributional model of \cite{Erk:10}.

Lewis and Steedman \cite{Lewis:13} take a standard logical approach to
semantics except that the relational constants used are derived from
distributional clustering. They use latent dirichlet allocation to
describe a probability distribution over semantic categories for
nouns, which are then combined with induced relation clusters to
generate a probability distribution over logical forms for a given
sentence.

\section{Conclusion}

We have presented two models for probabilistic semantics for natural
language. The first of these is a generalisation of Montague semantics
which assumes a distribution over models. We showed how restricting
the space of models makes it feasible to learn semantic
representations directly from data.

The second model, stochastic semantics, allows for the ambiguity of
natural language expressions to be handled naturally, whilst
sacrificing some properties of logical semantics.


% We have introduced probabilistic semantics for natural language, a
% framework for representing natural language meaning which extends
% Montague semantics by assuming a probability distribution over
% models. We have explored some properties of the framework, showing
% that it can represent fine-grained aspects of meaning, and that nice
% features of standard model-theoretic semantics, such as the simple
% representation of conjunction and disjunction are retained in our
% formalism.

% We also presented a restricted version of the semantics which makes it
% possible to learn meaning representations directly from data, and
% demonstrated that this formalism can learn the monotonicity of
% determiners from examples. We plan to apply this to practical tasks
% such as recognising textual entailment in future work.

In future work, we plan to apply our ideas to practical tasks such as
recognising textual entailment.
We also plan to further investigate the relationship between our models and
standard distributional semantic representations of meaning. This,
together with a development of the theory, may lead to interesting
new ways to describe probabilistic semantics that combines logical
aspects of meaning with those which are better represented
distributionally.

Finally, the examples presented here deal with a very small fragment
of natural language. There are many complex natural language phenomena
that have been dealt with successfully within the framework of
Montague semantics. This suggests that it should be possible to apply
probabilistic semantics to learn about a wide range of phenomena such
as quantifier scope ambiguity, intensional contexts, time and tense
and indexicals, amongst others.

% There are a number of directions in which the work may be developed. We are currently  exploring the application of probabilistic semantics to the general task of recognising textual entailment. This would involve learning from much larger datasets and provide a more stringent test of the practical application of the approach. More work is also needed to explore the properties of the distributional representations for words that are learnt by the model. 


\bibliographystyle{acl}
\bibliography{JW2012}
\end{document}
