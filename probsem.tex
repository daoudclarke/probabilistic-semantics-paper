%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{color}
\usepackage{listings}

\lstset{
         basicstyle=\footnotesize\ttfamily, % Standardschrift
         %numbers=left,               % Ort der Zeilennummern
         numberstyle=\tiny,          % Stil der Zeilennummern
         %stepnumber=2,               % Abstand zwischen den
         %Zeilennummern
         numbersep=5pt,              % Abstand der Nummern zum Text
         tabsize=2,                  % Groesse von Tabs
         extendedchars=true,         %
         breaklines=true,            % Zeilen werden Umgebrochen
         keywordstyle=\color{black},
         frame=b,         
 %        keywordstyle=[1]\textbf,    % Stil der Keywords
 %        keywordstyle=[2]\textbf,    %
 %        keywordstyle=[3]\textbf,    %
 %        keywordstyle=[4]\textbf,   \sqrt{\sqrt{}} %
         stringstyle=\color{white}\ttfamily, % Farbe der String
         showspaces=false,           % Leerzeichen anzeigen ?
         showtabs=false,             % Tabs anzeigen ?
         xleftmargin=17pt,
         framexleftmargin=17pt,
         framexrightmargin=5pt,
         framexbottommargin=4pt,
         %backgroundcolor=\color{lightgray},
         showstringspaces=false,      % Leerzeichen in Strings anzeigen
         language=prolog
 }

\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (Probabilistic Semantics for Natural Language)
/Author (Daoud Clarke, Bill Keller)}
\setcounter{secnumdepth}{0}  
 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Probabilistic Semantics for Natural Language}
\author{Daoud Clarke and Bill Keller\\
Department of Informatics\\
University of Sussex\\
Falmer, Brighton, UK\\
}
\maketitle
% \begin{abstract}
% \begin{quote}

% \end{quote}
% \end{abstract}


\section{Introduction}

Techniques which represent the meanings of words by the contexts in
which they occur have become an indispensible tool in natural language
processing. Such distributional representations of meaning typically
represent words as vectors or probability distributions over
contexts. They have been applied to a wide variety of tasks,
including sentiment classification~\cite{Bollegala2011}, word sense
disambiguation~\cite{miller-EtAl:2012:PAPERS,khapra-EtAl:2010:ACL}, PP
attachment~\cite{Calvo05distributionalthesaurus}, automatic confusion
set generation~\cite{xue-hwa:2012:PAPERS}, textual
entailment~\cite{berant-dagan-goldberger:2010:ACL}, co-reference
resolution~\cite{lee-EtAl:2012:EMNLP-CoNLL}, predicting semantic
compositionality~\cite{bergsma-EtAl:2010:EMNLP}, acquisition of
semantic lexicons~\cite{mcintosh:2010:EMNLP}, conversation
entailment~\cite{zhang-chai:2010:EMNLP}, semantic role
classification~\cite{zapirain-EtAl:2010:NAACLHLT}, lexical
substitution~\cite{szarvas-biemann-gurevych:2013:NAACL-HLT}, taxonomy
induction~\cite{fountain-lapata:2012:NAACL-HLT}, detection of visual
text~\cite{dodge-EtAl:2012:NAACL-HLT}, and parser
lexicalisation~\cite{rei-briscoe:2013:NAACL-HLT}.

Such representations of meaning seem to be in conflict with logical
approaches to representing meaning, such as Montague semantics or
Discourse Representation Theory \cite{Blackburn:05}. In these
approaches, a sentence is translated to a logical form, which is then
used for reasoning via theorem proving or model building.

Recently, effort has been focussed on resolving this conflict.

\section{Proposed Approach}

In standard model-theoretic semantics, one way to view the ``meaning''
of a logical sentence $s$ is as the set of all structures
$\mathcal{M}_s$ for which $s$ is true, also called models for
$s$. Then $s$ entails $t$ if $\mathcal{M}_s \subseteq
\mathcal{M}_t$. We propose an extension to this idea where we assume
that models occur randomly. Informally, we may assume a probability
distribution over the set of models; we can then estimate the
probability of a sentence $s$ as the sum of the probabilities of all
models $\mathcal{M}_s$. However, since $\mathcal{M}_s$ is normally an
infinite set, there is a more general definition that allows, for
example, each model to be equally likely (which is not possible if we
require a probability distribution over an infinite set).

Instead, we define $\Sigma$ as
$$\Sigma = \{\mathcal{M}_s : s \in L\}$$
where $L$ is the language of the logic. If $L$ contains a
propositional fragment then $\Sigma$ will define a $\sigma$-algebra on
the set of all structures $\mathcal{M}$. We then require $P$ to be a
probability measure on $\Sigma$. We write $P(s)$ as shorthand for
$P(\mathcal{M}_s)$, and interpret it as the probability that a sentence
is true, or more accurately, a degree of belief that $s$ is true.

We interpret the conditional probability $P(t|s)$ as the degree to
which $s$ entails $t$. This has the value 1 if $\mathcal{M}_s
\subseteq \mathcal{M}_t$ (the reverse implication holds if $P(X) >
0$ for all $X \neq \varnothing$).

In this paper, we consider the situation where the logical language
$L$ is restricted to that of logic programs, i.e.~Horn clauses with
universal quantification, and we use Herbrand models. In this case,
the distribution can be defined in terms of minimal models, as
demonstrated by Sato (\citeyear{Sato:95}), meaning that probabilities
of logical sentences can be computed efficiently. He also shows that
in this case distributions can be efficiently learnt from data using
the Expectation Maximisation (EM) algorithm. We demonstrate how this
can work for toy data consisting of natural language sentences.

\begin{figure}
\begin{lstlisting}
% Hidden variable
values(hidden, [h0, h1, h2, h3, h4]).

% Probabilities conditional on hidden variable
values(subject(_,_), [true, false]).
values(verb(_,_), [true, false]).
values(object(_,_), [true, false]).

sentence_truth(S, V, O, T) :-
	(S = true, V = true, O = true) -> T = true ; T = false.

all_true([], _).
all_true([[Subject, Verb, Object, Truth]|XT], Hidden) :-
	msw(subject(Subject, Hidden), ST),
	msw(verb(Verb, Hidden), VT),
	msw(object(Object, Hidden), OT),
	sentence_truth(ST, VT, OT, Truth),
	all_true(XT, Hidden).

is_true(X) :-
	msw(hidden, Hidden),
	all_true(X, Hidden).

:- learn([count(is_true([[cats, chase, dogs, true],[cats, chase, mice, true],[dogs, chase, cats, true],[dogs,chase,dogs,false]]), 1),
	count(is_true([[cats, eat, cheese, true],[cats, eat, cats, false]]), 1),
	count(is_true([[mice, eat, cheese, true],[cheese, eat, cheese, false]]), 1),
	count(is_true([[mice, eat, cats, false],[mice, chase, cheese, false]]), 1),
	count(is_true([[dogs, eat, cats, false],[dogs, chase, dogs, true]]), 1)]).
\end{lstlisting}
\caption{A PRISM program to learn probability distributions over
  natural language models.}
\end{figure}


\bibliographystyle{aaai}
\bibliography{JW2012}
\end{document}
