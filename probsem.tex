%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{starsem2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{color}
\usepackage{listings}
\usepackage{csvsimple}
\usepackage{titleref}
%\usepackage{hyperref}
%\usepackage{natbib}

\renewcommand{\theTitleReference}[2]{\emph{#2}}

%\renewcommand{\cite}{\citep}
%\newcommand{\citet}[1]{\citeauthor{#1} \shortcite{#1}}
\newcommand{\citet}[1]{\newcite{#1}}

\lstset{
         basicstyle=\footnotesize\ttfamily, % Standardschrift
         %numbers=left,               % Ort der Zeilennummern
         numberstyle=\tiny,          % Stil der Zeilennummern
         %stepnumber=2,               % Abstand zwischen den
         %Zeilennummern
         numbersep=5pt,              % Abstand der Nummern zum Text
         tabsize=2,                  % Groesse von Tabs
         extendedchars=true,         %
         breaklines=true,            % Zeilen werden Umgebrochen
         keywordstyle=\color{black},
         frame=b,         
 %        keywordstyle=[1]\textbf,    % Stil der Keywords
 %        keywordstyle=[2]\textbf,    %
 %        keywordstyle=[3]\textbf,    %
 %        keywordstyle=[4]\textbf,   \sqrt{\sqrt{}} %
         stringstyle=\color{white}\ttfamily, % Farbe der String
         showspaces=false,           % Leerzeichen anzeigen ?
         showtabs=false,             % Tabs anzeigen ?
         xleftmargin=17pt,
         framexleftmargin=17pt,
         framexrightmargin=5pt,
         framexbottommargin=4pt,
         %backgroundcolor=\color{lightgray},
         showstringspaces=false,      % Leerzeichen in Strings anzeigen
         language=prolog
 }


 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Towards Probabilistic Semantics for Natural Language}
% \author{Daoud Clarke and Bill Keller\\
% Department of Informatics\\
% University of Sussex\\
% Falmer, Brighton, UK\\
% }
\author{}
\maketitle
\begin{abstract}
% \begin{quote}
  % Distributional accounts of meaning represent words as vectors or
  % probability distributions over contexts. Such approaches differ from
  % compositional, model-theoretic treatments of meaning such as
  % Montague semantics. Recently, researchers have begun to address the
  % problem of developing a unified account of natural language
  % semantics that combines the strengths of both the distributional and
  % compositional approaches. This paper presents ongoing work to
  % develop a new formalism, {\em probabilistic semantics\/}, which
  % seeks to provide such a unifying account. Probabilistic semantics
  % extends standard model-theoretic accounts by assuming a probability
  % distribution over models. We sketch a probabilistic extension for a
  % Montague-style semantics and demonstrate its properties on simple
  % examples. We also propose an approach for learning semantics
  % directly from data, and discuss how the technique could be applied
  % to the task of recognising textual entailment.
% \end{quote}
\end{abstract}


\section{Introduction}

This paper argues for the adoption of probabilistic semantics as a
framework for combining distributional and logical semantics. The
success of lexical distributional representations of meaning has
recently lead to a concerted effort to describe compositionality
within distributional semantics
\cite{Widdows:08,Mitchell:08,Baroni2010,Garrette:11,Grefenstette:11,Socher:12,Lewis:13},
and efforts have been made to find a theoretical foundation for such
work \cite{Clarke:12,Kartsaklis:14}. In this paper, we argue that
existing work in probabilistic semantics
\cite{Gaifman:64,Nilsson:86,Sato:95} provides a rich seam of ideas
that can be applied to this problem.

The contributions of this paper are as follows:
\begin{itemize}
\item We propose a framework for natural language semantics which
  combines Montague semantics with probabilistic semantics by
  specifying a probability distribution over Montague-style models. As
  far as we are aware, this is the first time that probabilistic
  semantics has been applied to higher-order logic.
\item We describe ongoing work to construct a probabilistic Montague
  semantics in which meanings are learnt from data, and demonstrate
  some desirable properties of this formalism. We show that
  restricting the space of models makes learning feasible, whilst
  retaining many of the desirable properties of full Montague
  semantics.
\end{itemize}

Techniques which represent the meanings of words in terms of the contexts in
which they occur have become an indispensible tool in natural language
processing. Such distributional representations of meaning typically
represent words as vectors or probability distributions over
contexts. They have been applied to a wide variety of tasks,
including word sense
disambiguation~\cite{miller-EtAl:2012:PAPERS,khapra-EtAl:2010:ACL},
prepositional phrase
attachment~\cite{Calvo05distributionalthesaurus}, textual
entailment~\cite{berant-dagan-goldberger:2010:ACL}, co-reference
resolution~\cite{lee-EtAl:2012:EMNLP-CoNLL}, predicting semantic
compositionality~\cite{bergsma-EtAl:2010:EMNLP} and taxonomy
induction~\cite{fountain-lapata:2012:NAACL-HLT}.

Current distributional approaches to the representation of meaning
differ in many respects from compositional, model-theoretic treatments
such as Montague semantics or Discourse Representation Theory
\cite{Blackburn:05}. Whilst distributional approaches provide an
account of word meaning, but say little about how those meanings
combine, compositional approaches provide an account of the way in
which meanings combine to build a semantics for larger phrases, but
have little to say about the meanings of words or how they are
acquired.

Recently however, a number of researchers have begun to address the
problem of developing a unified account of natural language semantics
that combines the strengths of both the distributional and
model-theoretic approaches
\cite{Clarke:07,Coecke:10,Garrette:11,Lewis:13}.  In this paper, we
propose combining probabilistic semantics with Montague semantics,
seeking to provide such a unifying account.

%  The notion of a probability
% distribution over models has been proposed as a probabilistic
% semantics by \citet{Sato:95}, however to our knowledge it has not been
% extended to higher order logic, nor applied to natural language
% semantics.

The proposed approach satisfies each of the following desirable properties:
\begin{itemize}
\item It is strongly grounded in probability theory: no ad-hoc methods
  or heuristics are used to combine distributional semantics with
  logical semantics.
\item It provides the basis of an elegant extension of Montague
  semantics to the probabilistic domain: in particular it combines the
  strengths of a compositional, model-theoretic account of semantics,
  complete with a well-defined notion of entailment, with a
  distributional account of the meaning of expressions.
\item It maintains the tradition of logical approaches to semantics
  which are concerned with what models are satisfied rather than
  assuming a single model: this means that composition does not
  necessarily lead to a loss of meaning (for example by reducing the meaning of a
  sentence to a single truth value).
\item It admits an implementation in which probability distributions
  over models can potentially be learnt from data: the account is
  holistic, in that the learning of word meaning and composition are not
  separated, with the potential benefit of more flexible and accurate
  models of compositional distributional semantics.
\end{itemize}

In the following we sketch an approach to constructing a probabilistic
extension of Montague semantics and demonstrate its properties on
simple examples. We present a way of defining a probability
distribution over models in terms of  a random generation process over
interpretations.  We discuss how the technique could be applied to the
core task of textual entailment.

% I'm not sure about referring to "distribution semantics" as it may
% be confused with distributional semantics - perhaps just refer to it
% as Sato's framework? Or else note the distinction.

% Yes - I agree with that - removed reference to distribution semantics.


\section{Background}

\subsection{Montague Semantics}

In a series of papers in the early 1970s \cite{Montague1970a,Montague1970b,Montague1973} the linguist and philosopher Richard Montague spelled out a formal treatment of the semantics of natural language. Montague's conception of semantics was truth-conditional and model-theoretic. He considered that a fundamental aim of any adequate theory of semantics was ``to characterise the notions of a true sentence (under a given interpretation) and of entailment'' \cite{Montague1970b}. A central methodological component of Montague's approach was the {\em Principle of Compositionality\/}, which states that the meaning of an expression is a function of the meanings of its parts and the way they are combined syntactically.


We assume, as in Montague semantics, that natural language expressions
are parsed by a categorial grammar. Further, we assume that with every word there is
associated a function with a type. Let $\mathcal{T}$ be the set of types defined
recursively, such that:
\begin{description}
\item [Basic types:] $e,t\in \mathcal{T}$
\item[Complex types:]  if $\alpha, \beta\in \mathcal{T}$, then $\alpha/\beta\in \mathcal{T}$.
\end{description}
Nothing else is in $\mathcal{T}$. Note that the type $\alpha/\beta$ denotes the type of a function from type
$\alpha$ to type $\beta$.

We define the set $B$ of \emph{basic expressions} to be a set of
symbols denoting meanings of words. We assume that associated with
each $b\in B$ there is a type $\tau_b$. The set $\Lambda$ of
\emph{meaningful expressions}, and the extension of $\tau$ to $\Lambda$ are
defined recursively such that
\begin{itemize}
\item $B\subseteq \Lambda$
\item for every pair $\gamma,\delta\in \Lambda$ such that $\tau_\gamma
  = \alpha/\beta$ and $\tau_\delta = \alpha$, then $\gamma(\delta)\in
  \Lambda$, and $\tau_{\gamma(\delta)} = \beta$.
\end{itemize}
Let $\Lambda_\tau$ denote the set of meaningful expressions of type
$\tau$. A \emph{sentence} is a meaningful expression of type $t$.

% An
% \emph{interpretation} is a function $\phi$ which assigns to every
% sentence a value of true or false, denoted $\top, \bot$ respectively.

The set $D_\alpha$ of \emph{possible denotations} of type $\alpha$ is
defined recursively:
\begin{eqnarray*}
D_e &=& E\\
D_t &=& \{\bot,\top\}\\
D_{\alpha/\beta} &=& {D_\beta}^{D_\alpha}
\end{eqnarray*}
where $E$ is a set of \emph{entities}. Thus the denotation of a
complex type is a function between the denotations for the types from
which it is composed. An \emph{interpretation} is a pair $\langle E,
F\rangle$ such that $E$ is a non-empty set and $F$ is a function with
domain $B$ such that $F(b) \in D_{\tau_b}$ for all $b\in B$.

A meaningful expression $\gamma$ has the value $\phi(\gamma)$ in the
interpretation $\langle E, F\rangle$ defined recursively
\begin{itemize}
\item $\phi(b) = F(b)$ for $b\in B$
\item $\phi(\gamma(\delta)) = \phi(\gamma)(\phi(\delta))$ for $\gamma
  \in \Lambda_{\alpha/\beta}$ and $\delta \in \Lambda_\beta$.
\end{itemize}
A sentence $s$ is \emph{true} in interpretation $\langle E, F\rangle$
if $\phi(s) = \top$, otherwise it is \emph{false}.

A \emph{theory} is a set $T$ of pairs $(s,\hat{s})$, where $s$ is a
sentence and $\hat{s}\in\{\top,\bot\}$ is a truth value. A
\emph{model} for a theory $T$ is an interpretation $\langle E,
F\rangle$ such that $\phi(s) = \hat{s}$ for every sentence $s\in
T$. In this case we say that the model \emph{satisfies} $T$, and write
$\langle E, F\rangle \models T$.

\subsection{Probabilistic Semantics}

The idea of attaching probability to the truth of sentences is an old
one, and is related to the foundation of probability itself
\cite{Keynes:21,Los:55}. \citet{Gaifman:64} discusses probability
measures for first order calculus; the work of \citet{Sato:95}
concerns probability measures for logic programs.

% The idea that we adopt from these approaches is to associate
% probability measures with the space of models for the associated
% logic. To our knowledge, such measures have not been studied for
% higher order logics, which is what we consider here.

A key idea in probabilistic semantics is to extend standard
model-theoretic accounts of meaning by assuming a probability
distribution over models.  In model-theoretic semantics, one way to
view the meaning of a logical sentence $s$ is as the set of all
interpretations $\mathcal{M}_s$ for which $s$ is true. Interpretations
in $\mathcal{M}_s$ are also called {\em models for\/} $s$. Then $s$
{\em logically entails\/} $t$ if and only if any model for $s$ is a
model for $t$: i.e. $\mathcal{M}_s \subseteq
\mathcal{M}_t$. Probabilistic semantics extends this idea by assuming
that models occur randomly. Informally, we may assume a probability
distribution over the set of models. Given such a probability
distribution we can then estimate the probability of a sentence $s$ as
the sum of the probabilities of all models $\mathcal{M}_s$, for $s$.

Formally, for the sake of generality, probabilities are defined in
terms of a probability space $\langle \Omega, \sigma, \mu\rangle$,
where $\Omega$ is the set of all models, $\sigma$ is a sigma algebra
associated with theories and $\mu$ is a probability measure on
$\sigma$.

\section{Towards a Probabilistic Montague Semantics}

Let $\Omega$ be the set of all Montague-style interpretations, and
$\sigma$ be the sigma algebra generated by the set $\sigma_0$ of all
sets of models that satisfy some theory:
$$\sigma_0 = \{\mathcal{S} : \exists T (\forall\langle E,F\rangle\in \mathcal{S} (\langle E,F\rangle \models T))\}$$
Let $\mu$ be a probability measure on
$\sigma$; then $\langle\Omega,\sigma,\mu\rangle$ is a probability
space which describes the probability of theories. Let $M(T)$ denote
the set of all models for the theory $T$; the probability of $T$ is
$\mu(M(T))$.

% A \emph{theory} is a set $T$ of pairs $(s,\hat{s})$, where $s$ is a
% sentence and $\hat{s}\in\{\top,\bot\}$ is a truth value. A
% \emph{model} for a theory $T$ is an interpretation which assigns
% $\hat{s}$ to every sentence $s\in T$. In this case we say that the model \emph{satisfies} $T$. Let $\Omega$ be the set of all
% interpretations, and $\sigma$ be the sigma algebra consisting of the
% set of all sets of models that satisfy some theory. Let $\mu$ be a probability measure
% on $\sigma$; then $\langle\Omega,\sigma,\mu\rangle$ is a probability
% space which describes the probability of theories. Let $M(T)$ denote
% the set of all models for the theory $T$; the probability of $T$ is
% $\mu(M(T))$.

Given two sentences $s_1$ and $s_2$, we can compute the conditional
probability of $s_1$ given $s_2$ as
$$\frac{\mu(M(\{(s_1, \top), (s_2, \top)\}))}{\mu(M(\{(s_2,\top)\}))}$$
We interpret this conditional probability as the {\em degree to which $s_2$ entails $s_1$\/}. Note that if $s_2$ logically entails $s_1$ then the degree to which  $s_2$ entails $s_1$ is $1$.

% In this paper, we consider the situation where the logical language
% $L$ is restricted to that of logic programs, i.e.~Horn clauses with
% universal quantification, and we use Herbrand models. In this case,
% the distribution can be defined in terms of minimal models, as
% demonstrated by Sato (\citeyear{Sato:95}), meaning that probabilities
% of logical sentences can be computed efficiently. He also shows that
% in this case distributions can be efficiently learnt from data using
% the Expectation Maximisation (EM) algorithm. We demonstrate how this
% can work for toy data consisting of natural language sentences.

%\section{Learning}

%\section{Examples}

One can envisage many possible ways of describing probability measures
$\mu$. Our main goal is to be able to learn such measures from corpus
data, and we describe one possible approach to doing this in Section
\ref{section:distributions}. However, in this section, to demonstrate
the potential for our approach, we give examples in which measures are
given explicitly.
 
\subsection{Example: Fine-grained Meanings}

One distinguishing feature of distributional semantics is its ability
to describe fine-grained aspects of meaning: the continuous nature of
the space allows arbitrary degrees of similarity. We use the example
from \cite{Clark:08}, computing the similarity of the two sentences
\emph{John likes Mary} and \emph{John loves Mary}.

We assume the following types for basic expressions:
\begin{eqnarray*}
\mathit{john},\mathit{mary} & \in & \Lambda_{(e/t)/t}\\
\mathit{likes}, \mathit{loves} & \in & \Lambda_{(((e/t)/t)/((e/t)/t))/t}
\end{eqnarray*}
Proper nouns are interpreted as sets of sets of entities (functions
from functions from entities to truth values to truth values), while
verbs are functions from pairs of sets of sets of entities to truth
values.

\begin{table}
\begin{center}
\begin{tabular}{|l|l|l||c|c|c|c|}
\hline
\emph{subject} & \emph{verb} & \emph{object} & $m_1$ & $m_2$ & $m_3$ & $m_4$\\
\hline
john & likes & john & 1 & 1 & 1 & 1\\
john & likes & mary & 0 & 1 & 1 & 1\\
mary & likes & john & 1 & 0 & 0 & 1\\
mary & likes & mary & 0 & 0 & 1 & 1\\
john & loves & john & 1 & 1 & 1 & 1\\
john & loves & mary & 1 & 0 & 1 & 1\\
mary & loves & john & 1 & 1 & 1 & 1\\
mary & loves & mary & 1 & 0 & 1 & 1\\
\hline
\end{tabular}
\caption{Four possible models describing relationships between John
  and Mary.}
\end{center}
\label{table:models}
\end{table}

For simplicity of exposition, and to make the number of
interpretations we have to consider smaller, we will give some
properties which will be fixed for all interpretations under
consideration, i.e.~interpretations which do not satisfy the given
property will be given probability zero. For example, we assume that
the set $E$ of entities is fixed, $E = \{\mathbf{john},
\mathbf{mary}\}$, and that the interpretation of nouns is fixed:
$$\mathit{john}(x) = \begin{cases}
\top & \text{if } x(\mathbf{john}) = \top\\
\bot & \text{otherwise}
\end{cases}$$ i.e.~$\mathit{john}$ is the set of all sets containing
the entity $\mathbf{john}$, and similarly for $\mathit{mary}$, if we
interpret functions with range $\{\top,\bot\}$ as indicator
functions. We further assume that $\mathit{likes}$ and
$\mathit{loves}$ behave logically so that if ``John likes $x$'' is
true and ``Mary likes $x$'' is true then ``John and Mary like $x$'' is
also true (note that this isn't true for all verbs; consider ``John
and Mary played chess''). Similarly we assume that if ``$x$ likes
John'' and ``$x$ likes Mary'' are true then ``$x$ likes John and
Mary'' is true.

Four possible models $m_1, m_2, m_3, m_4$ under these assumptions are
described in Table \ref{table:models}. At this point our example
diverges from standard model-theoretic semantics. We assume a
probability distribution over the set of models: we assume that these
models have probabilities 0.1, 0.2, 0.3 and 0.4 respectively, and all
other models have probability zero.

We assume that the categorial grammar assigns the meaningful
expression $\mathit{likes}(\mathit{mary})(\mathit{john})$ to the
sentence ``John likes Mary''. The probability of this sentence is
\begin{eqnarray*}
\mu(M(\{\mathit{likes}(\mathit{mary})(\mathit{john})\})) &=&
\mu(\{m_2, m_3, m_4\})\\
&=&0.9
\end{eqnarray*}
Similarly, the models which are true for the sentence ``John loves
Mary'' are $\{m_1, m_3, m_4\}$, having probability 0.8. The models
which satisfy both of these sentences are $\{m_3, m_4\}$ with
probability 0.7, so the
degree to which ``John loves Mary'' entails ``John likes Mary'' is
$0.7/0.8$. Note that in this case there is no logical entailment, but
there is a high degree of entailment.

\subsection{Example: Conjunction and Disjunction}

Our formalism inherits the nice properties of model-theoretic
semantics with respect to conjunction and disjunction. We describe the
conjunction of nouns with the basic expression $\mathit{and}$ of type
$(((e/t)/t)/((e/t)/t))/((e/t)/t)$: it is a function taking two sets of
sets of entities, and returning another set of sets of
entities. Specifically we assume that it returns the intersection of
the two sets. Similarly $\mathit{or}$ is a function of the same type,
returning the union of the two sets.

Then $\mathit{and}(john)(mary)$ is the set of sets containing both
$\mathbf{john}$ and $\mathbf{mary}$. Assume the categorial grammar
gives the meaningful expression
$\mathit{likes}(\mathit{and}(\mathit{john})(\mathit{mary}))(\mathit{mary})$
to the sentence ``Mary likes John and Mary''. The set of models
satisfying this sentence is $\{m_4\}$, so this sentence has
probability 0.4. The set of models satisfying
$\mathit{likes}(\mathit{or}(\mathit{john})(\mathit{mary}))(\mathit{mary})$
for ``Mary likes John or Mary'' is $\{m_1, m_3, m_4\}$, so this
sentence has probability 0.8. Note that the logical entailments we
would expect from logical semantics are preserved: the degree to which
``Mary likes John and Mary'' entails ``Mary likes John'' is 1.

%  In this example, we
% assume that meanings are described by a measure space for each part of
% speech. Measure spaces for larger constituents are then constructed as
% product measures \cite{Taylor:06} of their constituent measure
% spaces. This idea is an extension of the observation of
% \citet{Keenan:85} that there is a Boolean algebra associated with each
% constituent.

% A measure space can be constructed for a particular part of speech
% directly from the distributional semantic representations of the
% associated words, or from an ontology with frequency information
% \cite{Clarke:07}.

\section{Learning Semantics}
\label{section:distributions}

Our main objective is to be able to learn semantics from data,
generalising lexical distributional semantics. In this section, we
sketch a proposal for how this may be achieved within our
framework. The central idea is to limit the number of denotations
under consideration, and define a probabilistic generative model for
interpretations.

Specifically we assume that $E$ is fixed. Let
$$\phi_\tau = \{\phi(\lambda) : \lambda\in \Lambda_\tau\}$$
be the set of denotations that occur with type $\tau$. We assume that
$F$ is constrained such that $|\phi_\tau| = n_\tau$ where $n_\tau$
is a constant for each type satisfying $n_\tau \le |D_\tau|$. This
restriction in the number of occurring denotations makes learning a
distribution over models feasible. We also assume that the occurring
denotations are ordered so that we can write $\phi_\tau =
\{d_{\tau,1}, d_{\tau,2}, \ldots d_{\tau, n_\tau}\}$.


We assume that denotations are generated with probabilities
conditionally independent given a random variable taking values from
some set $H$. This gives us the following process to generate $F$:
\begin{itemize}
\item Generate a hidden value $h\in H$
\item Generate a value $F(b) \in \phi_{\tau_b}$ for each $b\in B$ according to
  $P(d_{\tau_b,i}|b, h) = \theta_{b,i,h}$
\item For each pair of types $\alpha/\beta, \beta$:
\begin{itemize}
\item Generate a value $\phi(d_{\alpha/\beta,i}(d_{\alpha,j}))$
  according to $P(d_{\beta,k}|d_{\alpha/\beta,i}, d_{\alpha,j},h) = \theta_{\beta,i,j,k,h}$
\end{itemize}
\end{itemize}
The parameters to be learnt are the probability distributions
$\theta_{b,i,h}$ for each basic expression $b$ and hidden value $h$
over possible values $d_{\tau_b,i}$, and $\theta_{\beta,i,j,k,h}$ over
values $d_{\beta,k}$ for each function $d_{\alpha/\beta,i}$, argument
$d_{\alpha,j}$, and hidden value $h$.

\subsection{Inference}

Given a theory $T$ and parameters $\theta$, we can compute the
probability using the following algorithm:
\begin{itemize}
\item For each hidden variable $h$:
\begin{itemize}
\item Iterate over models for $T$. This can be done in a bottom-up
  fashion by first choosing the denotation for each basic expression,
  then recursively for complex expressions. Choices that are made have
  to be remembered, and any time a contradiction is detected, the
  model is abandoned.
\item The probability of each model given $h$ can be found by
  multiplying the parameters associated with each choice made in the
  previous step.
\end{itemize}
\end{itemize}

\subsection{Learning}

We can use Maximum Likelihood to estimate these parameters given a set
of observed theories $\mathcal{D} = \{T_1, T_2, \ldots T_N\}$. We look
for the parameters that maximize the likelihood
$$P(\mathcal{D}) = \prod_{n=1}^N P(T_i)$$
This can be maximised using gradient ascent or expectation
maximisation. Our current implementation uses the L-BFGS-B algorithm
in SciPy \cite{Zhu:97}.

\subsection{Application to Textual Entailment}

The task of recognising textual entailment \cite{Dagan:05} is to
determine, given two sentences $T$ (the \emph{text}) and $H$ (the
\emph{hypothesis}), whether $T$ {\em textually entails\/} $H$. It is important to note that the notion of
textual entailment is an informal one based on human judgments. This makes textual entailment much closer to the notion of degree of entailment available within our probabilistic approach than that of strict logical entailment.  The importance of textual entailment within natural language processing lies in its broad relevance to a range of tasks. It represents a core problem, a solution to which is required in order to develop applications in areas such as
information retrieval, machine translation, question answering and
summarisation.

Previous attempts to apply logical representations of meaning to this
task have applied {\em ad hoc\/} heuristics to handle the lack of robustness in
standard approaches. \citet{Bos:06} used model builders to construct a
model for $T$ and a model for $T$ and $H$ together and used the size
of these models as an indication of the degree of entailment. The
intuition here is that if the model for $T$ and $H$ together is larger
than that for just $T$, then $H$ contains a lot of knowledge not in
$T$ and there is a low degree of entailment.  This idea can be
directly related to our approach: it amounts to assuming the existence of a probability
distribution over models,
with larger models less probable than smaller ones.

Textual entailment datasets can also be used to learn the parameters
described in the previous section. A dataset typically consists of a
set of natural language sentence pairs, each annotated as to whether
entailment holds or not. Suppose we are given a pair $(T,H)$ with associated meaningful expressions $s_T$ and $s_H$. In the case that entailment holds for $T$ and $H$ then the dataset will contain the following theories as observations:
\[
\{(s_T,\top),(s_H,\top)\}, \{(s_T,\bot),(s_H,\top)\},\{(s_T,\bot),(s_H,\bot)\}
\]
In the case that entailment does not hold, on the other hand, then the dataset will contain just the following as an observation:
\[
\{(s_T,\top),(s_H,\bot)\}
\]

This
idea could be used as a supervised approach to the general task of recognising textual
entailment.  However, we leave the evaluation of this idea to future
work.

\section{Related Work}

%\cite{Clarke:07,Coecke:10,Garrette:11,Lewis:13}. 

\citet{Coecke:10} proposed a framework based on category-theoretic
similarities between vector spaces and pregroup grammars. Their
approach is arguably the most closely related to ours since it is also
founded in Montague semantics: words are treated as linear functions
between vector spaces. Each word is represented as an element of a
tensor product space, and a pregroup grammar is used to determine how
these are composed using the inner product. It was recently
demonstrated that this approach can be extended to allow the
simulation of predicate calculus using
tensors. \cite{Grefenstette:13}.

There are two main differences between their approach and ours. First,
in the language of Montague semantics, they assume a single
model. This means that composition necessarily loses information about
the composed words, and this is evident in the use of the inner
product for composition. Sentences are all described in a space of
fixed dimensionality, no matter how long or complex they are. In our
approach, a sentence describes a probability distribution over models,
which is an infinite set, allowing for the representation of arbitrary
complexity.

The second difference is that their approach is vector-based whereas
ours is probabilistic in nature. Thus their approach is more
closely related to approaches to lexical distributional semantics
which are vector based, such as latent semantic analysis
\cite{Deerwester:90} and random indexing \cite{Sahlgren:02}, whereas
ours is closer to probabilistic approaches such as latent Dirichlet
allocation \cite{Blei:03}.

\citet{Garrette:11} describe an approach to combining logical
semantics with distributional semantics using Markov Logic Networks
\cite{Richardson:06}. Sentences are parsed into logical form using
Boxer \cite{Bos:04}, and probabilistic rules are added using the
distributional model of \citet{Erk:10}.

\citet{Lewis:13} take a standard logical approach to semantics except
that the relational constants used are derived from distributional
clustering. They use latent dirichlet allocation to describe a
probability distribution over semantic categories for nouns, which are
then combined with induced relation clusters to generate a probability
distribution over logical forms for a given sentence.

\section{Conclusion}

We have introduced probabilistic semantics for natural language, a
framework for representing natural language meaning which extends
Montague semantics by assuming a probability distribution over
models. We have explored some properties of the framework, showing
that it can represent fine-grained aspects of meaning, and that nice
features of standard model-theoretic semantics, such as the simple
representation of conjunction and disjunction are retained in our
formalism.

We also presented a restricted version of the semantics which should
make it possible to learn meaning representations directly from data,
and discussed the application of this idea to the task of recognising
textual entailment. We plan to investigate practical implementations
of both these ideas in future work.

We also plan to investigate the relationship between our model and
standard distributional semantic representations of meaning. This,
together with a development of the theory, may lead to interesting
new ways to describe probabilistic semantics that combines logical
aspects of meaning with those which are better represented
distributionally.

Finally, the examples presented here deal with a very small fragment
of natural language. There are many complex natural language phenomena
that have been dealt with successfully within the framework of
Montague semantics. This suggests that it should be possible to apply
probabilistic semantics to learn about a wide range of phenomena such
as quantifier scope ambiguity, intensional contexts, time and tense
and indexicals, amongst others.

% There are a number of directions in which the work may be developed. We are currently  exploring the application of probabilistic semantics to the general task of recognising textual entailment. This would involve learning from much larger datasets and provide a more stringent test of the practical application of the approach. More work is also needed to explore the properties of the distributional representations for words that are learnt by the model. 


\bibliographystyle{acl}
\bibliography{JW2012}
\end{document}
